{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f71edeb-f750-400d-8b8e-5ed1652b14e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:   0%|                         | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2006Blood.Diamond chunk 1\n",
      "âœ… Scored: 2006Blood.Diamond chunk 2\n",
      "âœ… Scored: 2006Blood.Diamond chunk 3\n",
      "âœ… Scored: 2006Blood.Diamond chunk 4\n",
      "âœ… Scored: 2006Blood.Diamond chunk 5\n",
      "âœ… Scored: 2006Blood.Diamond chunk 6\n",
      "âœ… Scored: 2006Blood.Diamond chunk 7\n",
      "âœ… Scored: 2006Blood.Diamond chunk 8\n",
      "âœ… Scored: 2006Blood.Diamond chunk 9\n",
      "âœ… Scored: 2006Blood.Diamond chunk 10\n",
      "âœ… Scored: 2006Blood.Diamond chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:   5%|â–Š                | 1/20 [00:47<14:55, 47.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2005The.Constant.Gardener chunk 1\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 2\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 3\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 4\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 5\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 6\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 7\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 8\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 9\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 10\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 11\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 12\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  10%|â–ˆâ–‹               | 2/20 [01:26<12:51, 42.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 1\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 2\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 3\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 4\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 5\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  15%|â–ˆâ–ˆâ–Œ              | 3/20 [01:45<08:58, 31.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2009Avatar chunk 1\n",
      "âœ… Scored: 2009Avatar chunk 2\n",
      "âœ… Scored: 2009Avatar chunk 3\n",
      "âœ… Scored: 2009Avatar chunk 4\n",
      "âœ… Scored: 2009Avatar chunk 5\n",
      "âœ… Scored: 2009Avatar chunk 6\n",
      "âœ… Scored: 2009Avatar chunk 7\n",
      "âœ… Scored: 2009Avatar chunk 8\n",
      "âœ… Scored: 2009Avatar chunk 9\n",
      "âœ… Scored: 2009Avatar chunk 10\n",
      "âœ… Scored: 2009Avatar chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  20%|â–ˆâ–ˆâ–ˆâ–             | 4/20 [02:20<08:46, 32.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2012The.Hunger.Games chunk 1\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 2\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 3\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 4\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 5\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 6\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 7\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 5/20 [02:43<07:20, 29.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1984Ghostbusters chunk 1\n",
      "âœ… Scored: 1984Ghostbusters chunk 2\n",
      "âœ… Scored: 1984Ghostbusters chunk 3\n",
      "âœ… Scored: 1984Ghostbusters chunk 4\n",
      "âœ… Scored: 1984Ghostbusters chunk 5\n",
      "âœ… Scored: 1984Ghostbusters chunk 6\n",
      "âœ… Scored: 1984Ghostbusters chunk 7\n",
      "âœ… Scored: 1984Ghostbusters chunk 8\n",
      "âœ… Scored: 1984Ghostbusters chunk 9\n",
      "âœ… Scored: 1984Ghostbusters chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 6/20 [03:13<06:55, 29.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1978Superman chunk 1\n",
      "âœ… Scored: 1978Superman chunk 2\n",
      "âœ… Scored: 1978Superman chunk 3\n",
      "âœ… Scored: 1978Superman chunk 4\n",
      "âœ… Scored: 1978Superman chunk 5\n",
      "âœ… Scored: 1978Superman chunk 6\n",
      "âœ… Scored: 1978Superman chunk 7\n",
      "âœ… Scored: 1978Superman chunk 8\n",
      "âœ… Scored: 1978Superman chunk 9\n",
      "âœ… Scored: 1978Superman chunk 10\n",
      "âœ… Scored: 1978Superman chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 7/20 [03:45<06:33, 30.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2008The.Hurt.Locker chunk 1\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 2\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 3\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 4\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 5\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 6\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 7\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 8\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 8/20 [04:11<05:49, 29.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 1\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 2\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 3\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 4\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 5\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 6\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 7\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 8\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 9\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 10\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 11\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 9/20 [04:49<05:48, 31.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 1\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 2\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 3\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 4\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 5\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 6\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 7\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 8\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 9\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 10/20 [05:18<05:09, 30.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2018Black.Panther chunk 1\n",
      "âœ… Scored: 2018Black.Panther chunk 2\n",
      "âœ… Scored: 2018Black.Panther chunk 3\n",
      "âœ… Scored: 2018Black.Panther chunk 4\n",
      "âœ… Scored: 2018Black.Panther chunk 5\n",
      "âœ… Scored: 2018Black.Panther chunk 6\n",
      "âœ… Scored: 2018Black.Panther chunk 7\n",
      "âœ… Scored: 2018Black.Panther chunk 8\n",
      "âœ… Scored: 2018Black.Panther chunk 9\n",
      "âœ… Scored: 2018Black.Panther chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 11/20 [05:49<04:38, 30.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2021Dont.Look.Up chunk 1\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 2\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 3\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 4\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 5\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 6\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 7\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 8\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 9\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 10\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 11\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 12\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 13\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 14\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 15\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 16\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 12/20 [06:49<05:19, 39.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1982First.Blood chunk 1\n",
      "âœ… Scored: 1982First.Blood chunk 2\n",
      "âœ… Scored: 1982First.Blood chunk 3\n",
      "âœ… Scored: 1982First.Blood chunk 4\n",
      "âœ… Scored: 1982First.Blood chunk 5\n",
      "âœ… Scored: 1982First.Blood chunk 6\n",
      "âœ… Scored: 1982First.Blood chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/20 [07:10<03:58, 34.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2019Joker chunk 1\n",
      "âœ… Scored: 2019Joker chunk 2\n",
      "âœ… Scored: 2019Joker chunk 3\n",
      "âœ… Scored: 2019Joker chunk 4\n",
      "âœ… Scored: 2019Joker chunk 5\n",
      "âœ… Scored: 2019Joker chunk 6\n",
      "âœ… Scored: 2019Joker chunk 7\n",
      "âœ… Scored: 2019Joker chunk 8\n",
      "âœ… Scored: 2019Joker chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/20 [07:40<03:16, 32.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2006Night.at.the.Museum chunk 1\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 2\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 3\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 4\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 5\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 6\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 7\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 8\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 9\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/20 [08:12<02:42, 32.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1976Rocky.I chunk 1\n",
      "âœ… Scored: 1976Rocky.I chunk 2\n",
      "âœ… Scored: 1976Rocky.I chunk 3\n",
      "âœ… Scored: 1976Rocky.I chunk 4\n",
      "âœ… Scored: 1976Rocky.I chunk 5\n",
      "âœ… Scored: 1976Rocky.I chunk 6\n",
      "âœ… Scored: 1976Rocky.I chunk 7\n",
      "âœ… Scored: 1976Rocky.I chunk 8\n",
      "âœ… Scored: 1976Rocky.I chunk 9\n",
      "âœ… Scored: 1976Rocky.I chunk 10\n",
      "âœ… Scored: 1976Rocky.I chunk 11\n",
      "âœ… Scored: 1976Rocky.I chunk 12\n",
      "âœ… Scored: 1976Rocky.I chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 16/20 [08:54<02:22, 35.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2005V.for.Vendetta chunk 1\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 2\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 3\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 4\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 5\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 6\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 7\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 8\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 9\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 10\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 11\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 12\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 13\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 14\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 15\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17/20 [09:56<02:10, 43.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2017Paddington.2 chunk 1\n",
      "âœ… Scored: 2017Paddington.2 chunk 2\n",
      "âœ… Scored: 2017Paddington.2 chunk 3\n",
      "âœ… Scored: 2017Paddington.2 chunk 4\n",
      "âœ… Scored: 2017Paddington.2 chunk 5\n",
      "âœ… Scored: 2017Paddington.2 chunk 6\n",
      "âœ… Scored: 2017Paddington.2 chunk 7\n",
      "âœ… Scored: 2017Paddington.2 chunk 8\n",
      "âœ… Scored: 2017Paddington.2 chunk 9\n",
      "âœ… Scored: 2017Paddington.2 chunk 10\n",
      "âœ… Scored: 2017Paddington.2 chunk 11\n",
      "âœ… Scored: 2017Paddington.2 chunk 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 18/20 [10:42<01:28, 44.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1985Back.To.The.Future chunk 1\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 2\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 3\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 4\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 5\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 6\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 7\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 8\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 9\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 10\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 19/20 [11:15<00:40, 40.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2013The.Purge chunk 1\n",
      "âœ… Scored: 2013The.Purge chunk 2\n",
      "âœ… Scored: 2013The.Purge chunk 3\n",
      "âœ… Scored: 2013The.Purge chunk 4\n",
      "âœ… Scored: 2013The.Purge chunk 5\n",
      "âœ… Scored: 2013The.Purge chunk 6\n",
      "âœ… Scored: 2013The.Purge chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [13:35<00:00, 40.77s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Claude API setup ===\n",
    "api_key = \"api-key\"\n",
    "api_url = \"https://api.anthropic.com/v1/messages\"\n",
    "model_name = \"claude-3-haiku-20240307\"\n",
    "\n",
    "headers = {\n",
    "    \"x-api-key\": api_key,\n",
    "    \"anthropic-version\": \"2023-06-01\",\n",
    "    \"content-type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# === Paths ===\n",
    "base = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "subs_dir = base / \"json_benchmark\"\n",
    "summaries_dir = base / \"summaries\"\n",
    "output_dir = base / \"scored_progressive_women\"\n",
    "prompts_dir = base / \"prompts_progressive_women\"\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "prompts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "matches_df = pd.read_csv(base / \"matches_benchmark.csv\")\n",
    "\n",
    "# === Prompt template ===\n",
    "def create_progressive_women_prompt(summary, chunk):\n",
    "    return f\"\"\"\n",
    "You are evaluating how progressively women are portrayed in a film based on its plot summary and dialogue.\n",
    "\n",
    "Your task is to classify the film into one of the following categories based on the roles, agency, and representation of women:\n",
    "\n",
    "- **Yes**: The film offers a clearly progressive portrayal of women. Female characters are active agents, hold central narrative roles, and are treated with depth and respect.\n",
    "- **Mixed**: The portrayal includes both progressive and stereotypical elements. Some female characters may show agency, while others may be sidelined or underdeveloped.\n",
    "- **No**: The film does not offer a progressive portrayal. Female characters are absent, objectified, stereotyped, or lack meaningful agency.\n",
    "\n",
    "Please:\n",
    "1. Choose the best label: \"yes\", \"mixed\", or \"no\"\n",
    "2. Provide a short justification (1â€“2 sentences)\n",
    "3. Include a confidence score between 0.0 and 1.0\n",
    "\n",
    "Return your answer in this exact JSON format:\n",
    "\n",
    "{{\n",
    "  \"progressive_women\": \"...\",\n",
    "  \"confidence\": ...,\n",
    "  \"explanation\": \"...\"\n",
    "}}\n",
    "\n",
    "Film Summary:\n",
    "\\\"\\\"\\\"{summary}\\\"\\\"\\\"\n",
    "\n",
    "Dialogue:\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
    "\"\"\".strip()\n",
    "\n",
    "# === Claude API call ===\n",
    "def call_claude(prompt):\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0,\n",
    "        \"system\": \"You are a careful film analyst. Always return exactly and only the required JSON.\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 429:\n",
    "        print(\"âš ï¸ Rate limit hit. Waiting...\")\n",
    "        time.sleep(60)\n",
    "        return call_claude(prompt)\n",
    "    elif response.status_code != 200:\n",
    "        print(f\"âŒ API Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        content = response.json()[\"content\"][0][\"text\"]\n",
    "        return json.loads(content)\n",
    "    except Exception:\n",
    "        print(f\"âš ï¸ JSON parsing error:\\n{response.text}\")\n",
    "        return None\n",
    "\n",
    "# === Chunk subtitles ===\n",
    "def chunk_dialogue(subs, chunk_size=5000, overlap=200):\n",
    "    text_blocks = [line.get(\"text\", \"\").strip() for line in subs if line.get(\"text\")]\n",
    "    full_text = \" \".join(text_blocks)\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(full_text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(full_text[start:end])\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# === Scoring loop ===\n",
    "for _, row in tqdm(matches_df.iterrows(), total=len(matches_df), desc=\"Scoring progressive_women\"):\n",
    "    filename = row[\"subtitle_filename\"]\n",
    "    json_path = subs_dir / f\"{filename}.json\"\n",
    "    summary_path = summaries_dir / f\"{filename}.srt_summary.txt\"\n",
    "\n",
    "    if not json_path.exists() or not summary_path.exists():\n",
    "        print(f\"âš ï¸ Missing files for: {filename}\")\n",
    "        continue\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        subs = json.load(f)\n",
    "\n",
    "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = f.read().strip()\n",
    "\n",
    "    chunks = chunk_dialogue(subs)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_path = output_dir / f\"{filename}_chunk{i+1}_progressive_women.json\"\n",
    "        prompt_path = prompts_dir / f\"{filename}_chunk{i+1}_progressive_women_prompt.txt\"\n",
    "\n",
    "        if output_path.exists():\n",
    "            print(f\"ðŸŸ¡ Already scored: {filename} chunk {i+1}\")\n",
    "            continue\n",
    "\n",
    "        prompt = create_progressive_women_prompt(summary, chunk)\n",
    "\n",
    "        with open(prompt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(prompt)\n",
    "\n",
    "        result = call_claude(prompt)\n",
    "        if result:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "            print(f\"âœ… Scored: {filename} chunk {i+1}\")\n",
    "        else:\n",
    "            print(f\"âŒ Failed: {filename} chunk {i+1}\")\n",
    "\n",
    "        time.sleep(1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2341e259-8878-4e6e-9088-aa827122b82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved aggregated results to: /Users/cedricroetheli/Desktop/Benchmark/model_progressive_women_output.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# === Paths ===\n",
    "base_path = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "scored_dir = base_path / \"scored_progressive_women\"\n",
    "output_csv_path = base_path / \"model_progressive_women_output.csv\"\n",
    "\n",
    "# === Aggregate Results ===\n",
    "aggregated_results = []\n",
    "\n",
    "# Group chunk files by movie\n",
    "movie_files = defaultdict(list)\n",
    "for file in scored_dir.glob(\"*_chunk*_progressive_women.json\"):\n",
    "    movie_id = file.name.split(\"_chunk\")[0]\n",
    "    movie_files[movie_id].append(file)\n",
    "\n",
    "# Process each movie's chunks\n",
    "for movie_id, files in movie_files.items():\n",
    "    label_counts = Counter()\n",
    "    confidence_sums = defaultdict(float)\n",
    "    confidence_counts = defaultdict(int)\n",
    "    explanations = []\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                label = data.get(\"progressive_women\", \"\").strip().lower()\n",
    "                confidence = float(data.get(\"confidence\", 0.0))\n",
    "                explanation = data.get(\"explanation\", \"\")\n",
    "\n",
    "                label_counts[label] += 1\n",
    "                confidence_sums[label] += confidence\n",
    "                confidence_counts[label] += 1\n",
    "                explanations.append(f\"{label} ({confidence:.2f}): {explanation}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error in file {file.name}: {e}\")\n",
    "\n",
    "    # Compute average confidence per label\n",
    "    avg_confidences = {\n",
    "        label: round(confidence_sums[label] / confidence_counts[label], 3)\n",
    "        for label in label_counts\n",
    "    }\n",
    "\n",
    "    # Weighted vote = label with highest total confidence\n",
    "    weighted_vote = max(confidence_sums.items(), key=lambda x: x[1])[0] if confidence_sums else None\n",
    "\n",
    "    aggregated_results.append({\n",
    "        \"subtitle_filename\": movie_id,\n",
    "        \"label_counts\": dict(label_counts),\n",
    "        \"avg_confidences\": avg_confidences,\n",
    "        \"weighted_vote\": weighted_vote,\n",
    "        \"explanations\": explanations\n",
    "    })\n",
    "\n",
    "# === Save to CSV ===\n",
    "df = pd.DataFrame(aggregated_results)\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"âœ… Saved aggregated results to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6acad5ad-a10c-493c-a59e-3d89d3277b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Evaluation complete:\n",
      "âœ… Correct: 16/20\n",
      "ðŸ“Š Accuracy: 80.00%\n",
      "ðŸ“ Saved to: /Users/cedricroetheli/Desktop/Benchmark/progressive_women_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "base = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "truth_path = base / \"benchmark_final.csv\"\n",
    "model_path = base / \"model_progressive_women_output.csv\"\n",
    "output_path = base / \"progressive_women_evaluation.csv\"\n",
    "\n",
    "# === Load Data ===\n",
    "truth_df = pd.read_csv(truth_path)\n",
    "model_df = pd.read_csv(model_path)\n",
    "\n",
    "# === Normalize filenames ===\n",
    "truth_df[\"subtitle_filename\"] = truth_df[\"subtitle_filename\"].str.strip()\n",
    "model_df[\"subtitle_filename\"] = model_df[\"subtitle_filename\"].str.strip()\n",
    "\n",
    "# === Label Mapping for Consistency ===\n",
    "label_map = {\n",
    "    \"yes\": \"yes\",\n",
    "    \"no\": \"no\",\n",
    "    \"mixed\": \"mixed\",\n",
    "    \"partially\": \"mixed\",\n",
    "    \"partly\": \"mixed\"\n",
    "}\n",
    "\n",
    "def normalize_label(label):\n",
    "    if pd.isna(label):\n",
    "        return \"\"\n",
    "    label = label.lower().strip()\n",
    "    return label_map.get(label, label)\n",
    "\n",
    "def normalize_set(label_str):\n",
    "    if pd.isna(label_str):\n",
    "        return set()\n",
    "    parts = [normalize_label(part) for part in label_str.split(\"|\")]\n",
    "    return set(parts)\n",
    "\n",
    "truth_df[\"progressive_women_set\"] = truth_df[\"progressive_women\"].apply(normalize_set)\n",
    "model_df[\"normalized_vote\"] = model_df[\"weighted_vote\"].apply(normalize_label)\n",
    "\n",
    "# === Merge and Evaluate ===\n",
    "merged_df = pd.merge(model_df, truth_df, on=\"subtitle_filename\", how=\"inner\")\n",
    "merged_df[\"is_correct\"] = merged_df.apply(\n",
    "    lambda row: row[\"normalized_vote\"] in row[\"progressive_women_set\"], axis=1\n",
    ")\n",
    "\n",
    "# === Output CSV ===\n",
    "evaluation_df = merged_df[[\n",
    "    \"subtitle_filename\", \"progressive_women\", \"normalized_vote\", \"is_correct\"\n",
    "]].copy()\n",
    "evaluation_df.columns = [\"movie\", \"benchmark_progressive_women\", \"model_progressive_women\", \"is_correct\"]\n",
    "evaluation_df.to_csv(output_path, index=False)\n",
    "\n",
    "# === Summary ===\n",
    "total = len(evaluation_df)\n",
    "correct = evaluation_df[\"is_correct\"].sum()\n",
    "accuracy = correct / total if total else 0\n",
    "\n",
    "print(f\"ðŸŽ¯ Evaluation complete:\")\n",
    "print(f\"âœ… Correct: {correct}/{total}\")\n",
    "print(f\"ðŸ“Š Accuracy: {accuracy:.2%}\")\n",
    "print(f\"ðŸ“ Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c27151-2644-4612-b985-a4c4fda6cd45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_clean_env_py310)",
   "language": "python",
   "name": "my_clean_env_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
