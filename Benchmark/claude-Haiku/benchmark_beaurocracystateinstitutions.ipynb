{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3480459-ff36-4380-a68d-4d0f8fce3327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:   0%|                     | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2006Blood.Diamond chunk 1\n",
      "âœ… Scored: 2006Blood.Diamond chunk 2\n",
      "âœ… Scored: 2006Blood.Diamond chunk 3\n",
      "âœ… Scored: 2006Blood.Diamond chunk 4\n",
      "âœ… Scored: 2006Blood.Diamond chunk 5\n",
      "âœ… Scored: 2006Blood.Diamond chunk 6\n",
      "âœ… Scored: 2006Blood.Diamond chunk 7\n",
      "âœ… Scored: 2006Blood.Diamond chunk 8\n",
      "âœ… Scored: 2006Blood.Diamond chunk 9\n",
      "âœ… Scored: 2006Blood.Diamond chunk 10\n",
      "âœ… Scored: 2006Blood.Diamond chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:   5%|â–‹            | 1/20 [00:54<17:06, 54.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2005The.Constant.Gardener chunk 1\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 2\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 3\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 4\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 5\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 6\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 7\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 8\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 9\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 10\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 11\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 12\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  10%|â–ˆâ–Ž           | 2/20 [01:31<13:13, 44.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 1\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 2\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 3\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 4\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 5\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  15%|â–ˆâ–‰           | 3/20 [01:48<09:04, 32.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2009Avatar chunk 1\n",
      "âœ… Scored: 2009Avatar chunk 2\n",
      "âœ… Scored: 2009Avatar chunk 3\n",
      "âœ… Scored: 2009Avatar chunk 4\n",
      "âœ… Scored: 2009Avatar chunk 5\n",
      "âœ… Scored: 2009Avatar chunk 6\n",
      "âœ… Scored: 2009Avatar chunk 7\n",
      "âœ… Scored: 2009Avatar chunk 8\n",
      "âœ… Scored: 2009Avatar chunk 9\n",
      "âœ… Scored: 2009Avatar chunk 10\n",
      "âœ… Scored: 2009Avatar chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  20%|â–ˆâ–ˆâ–Œ          | 4/20 [02:19<08:25, 31.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2012The.Hunger.Games chunk 1\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 2\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 3\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 4\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 5\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 6\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 7\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  25%|â–ˆâ–ˆâ–ˆâ–Ž         | 5/20 [02:53<08:04, 32.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1984Ghostbusters chunk 1\n",
      "âœ… Scored: 1984Ghostbusters chunk 2\n",
      "âœ… Scored: 1984Ghostbusters chunk 3\n",
      "âœ… Scored: 1984Ghostbusters chunk 4\n",
      "âœ… Scored: 1984Ghostbusters chunk 5\n",
      "âœ… Scored: 1984Ghostbusters chunk 6\n",
      "âœ… Scored: 1984Ghostbusters chunk 7\n",
      "âœ… Scored: 1984Ghostbusters chunk 8\n",
      "âœ… Scored: 1984Ghostbusters chunk 9\n",
      "âœ… Scored: 1984Ghostbusters chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  30%|â–ˆâ–ˆâ–ˆâ–‰         | 6/20 [03:23<07:20, 31.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1978Superman chunk 1\n",
      "âœ… Scored: 1978Superman chunk 2\n",
      "âœ… Scored: 1978Superman chunk 3\n",
      "âœ… Scored: 1978Superman chunk 4\n",
      "âœ… Scored: 1978Superman chunk 5\n",
      "âœ… Scored: 1978Superman chunk 6\n",
      "âœ… Scored: 1978Superman chunk 7\n",
      "âœ… Scored: 1978Superman chunk 8\n",
      "âœ… Scored: 1978Superman chunk 9\n",
      "âœ… Scored: 1978Superman chunk 10\n",
      "âœ… Scored: 1978Superman chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 7/20 [03:56<06:55, 31.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2008The.Hurt.Locker chunk 1\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 2\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 3\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 4\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 5\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 6\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 7\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 8\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 8/20 [04:23<06:06, 30.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 1\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 2\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 3\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 4\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 5\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 6\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 7\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 8\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 9\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 10\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 11\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 9/20 [04:58<05:49, 31.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 1\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 2\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 3\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 4\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 5\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 6\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 7\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 8\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 9\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 10/20 [05:27<05:09, 30.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2018Black.Panther chunk 1\n",
      "âœ… Scored: 2018Black.Panther chunk 2\n",
      "âœ… Scored: 2018Black.Panther chunk 3\n",
      "âœ… Scored: 2018Black.Panther chunk 4\n",
      "âœ… Scored: 2018Black.Panther chunk 5\n",
      "âœ… Scored: 2018Black.Panther chunk 6\n",
      "âœ… Scored: 2018Black.Panther chunk 7\n",
      "âœ… Scored: 2018Black.Panther chunk 8\n",
      "âœ… Scored: 2018Black.Panther chunk 9\n",
      "âœ… Scored: 2018Black.Panther chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 11/20 [06:07<05:04, 33.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2021Dont.Look.Up chunk 1\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 2\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 3\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 4\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 5\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 6\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 7\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 8\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 9\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 10\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 11\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 12\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 13\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 14\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 15\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 16\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/20 [06:58<05:12, 39.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1982First.Blood chunk 1\n",
      "âœ… Scored: 1982First.Blood chunk 2\n",
      "âœ… Scored: 1982First.Blood chunk 3\n",
      "âœ… Scored: 1982First.Blood chunk 4\n",
      "âœ… Scored: 1982First.Blood chunk 5\n",
      "âœ… Scored: 1982First.Blood chunk 6\n",
      "âœ… Scored: 1982First.Blood chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13/20 [07:19<03:54, 33.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2019Joker chunk 1\n",
      "âœ… Scored: 2019Joker chunk 2\n",
      "âœ… Scored: 2019Joker chunk 3\n",
      "âœ… Scored: 2019Joker chunk 4\n",
      "âœ… Scored: 2019Joker chunk 5\n",
      "âœ… Scored: 2019Joker chunk 6\n",
      "âœ… Scored: 2019Joker chunk 7\n",
      "âœ… Scored: 2019Joker chunk 8\n",
      "âœ… Scored: 2019Joker chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14/20 [07:46<03:09, 31.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2006Night.at.the.Museum chunk 1\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 2\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 3\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 4\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 5\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 6\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 7\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 8\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 9\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 15/20 [08:18<02:38, 31.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1976Rocky.I chunk 1\n",
      "âœ… Scored: 1976Rocky.I chunk 2\n",
      "âœ… Scored: 1976Rocky.I chunk 3\n",
      "âœ… Scored: 1976Rocky.I chunk 4\n",
      "âœ… Scored: 1976Rocky.I chunk 5\n",
      "âœ… Scored: 1976Rocky.I chunk 6\n",
      "âœ… Scored: 1976Rocky.I chunk 7\n",
      "âœ… Scored: 1976Rocky.I chunk 8\n",
      "âœ… Scored: 1976Rocky.I chunk 9\n",
      "âœ… Scored: 1976Rocky.I chunk 10\n",
      "âœ… Scored: 1976Rocky.I chunk 11\n",
      "âœ… Scored: 1976Rocky.I chunk 12\n",
      "âœ… Scored: 1976Rocky.I chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 16/20 [08:56<02:14, 33.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2005V.for.Vendetta chunk 1\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 2\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 3\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 4\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 5\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 6\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 7\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 8\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 9\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 10\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 11\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 12\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 13\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 14\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 15\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 17/20 [09:42<01:51, 37.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2017Paddington.2 chunk 1\n",
      "âœ… Scored: 2017Paddington.2 chunk 2\n",
      "âœ… Scored: 2017Paddington.2 chunk 3\n",
      "âœ… Scored: 2017Paddington.2 chunk 4\n",
      "âœ… Scored: 2017Paddington.2 chunk 5\n",
      "âœ… Scored: 2017Paddington.2 chunk 6\n",
      "âœ… Scored: 2017Paddington.2 chunk 7\n",
      "âœ… Scored: 2017Paddington.2 chunk 8\n",
      "âœ… Scored: 2017Paddington.2 chunk 9\n",
      "âœ… Scored: 2017Paddington.2 chunk 10\n",
      "âœ… Scored: 2017Paddington.2 chunk 11\n",
      "âœ… Scored: 2017Paddington.2 chunk 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 18/20 [10:17<01:13, 36.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1985Back.To.The.Future chunk 1\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 2\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 3\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 4\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 5\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 6\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 7\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 8\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 9\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 10\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 19/20 [10:47<00:34, 34.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2013The.Purge chunk 1\n",
      "âœ… Scored: 2013The.Purge chunk 2\n",
      "âœ… Scored: 2013The.Purge chunk 3\n",
      "âœ… Scored: 2013The.Purge chunk 4\n",
      "âœ… Scored: 2013The.Purge chunk 5\n",
      "âœ… Scored: 2013The.Purge chunk 6\n",
      "âœ… Scored: 2013The.Purge chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [11:19<00:00, 33.96s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Claude API setup ===\n",
    "api_key = \"api-key\"  # ðŸ” Replace with your key\n",
    "api_url = \"https://api.anthropic.com/v1/messages\"\n",
    "model_name = \"claude-3-haiku-20240307\"\n",
    "\n",
    "headers = {\n",
    "    \"x-api-key\": api_key,\n",
    "    \"anthropic-version\": \"2023-06-01\",\n",
    "    \"content-type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# === Paths ===\n",
    "base = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "subs_dir = base / \"json_benchmark\"\n",
    "summaries_dir = base / \"summaries\"\n",
    "output_dir = base / \"scored_institutions_position\"\n",
    "prompts_dir = base / \"prompts_institutions_position\"\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "prompts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Load match file ===\n",
    "matches_df = pd.read_csv(\"matches_benchmark.csv\")\n",
    "\n",
    "# === Prompt template ===\n",
    "def create_institutions_position_prompt(summary, chunk):\n",
    "    return f\"\"\"\n",
    "You are analyzing how a film portrays bureaucracy, the state, and other formal institutions.\n",
    "\n",
    "Your task is to classify the filmâ€™s overall stance toward these institutions into one of three categories:\n",
    "\n",
    "- Opposed: Institutions are portrayed as corrupt, repressive, harmful, or adversarial. The narrative or characters resist or challenge them.\n",
    "- Neutral: Institutions are present but play a limited, mixed, or ambiguous role. They are neither clearly positive nor negative.\n",
    "- Supported: Institutions are shown as legitimate, effective, or beneficial. The story affirms their role or value.\n",
    "\n",
    "Please:\n",
    "1. Select one of the three stance labels: \"opposed\", \"neutral\", or \"supported\"\n",
    "2. Provide a brief explanation (1â€“2 sentences max)\n",
    "3. Include a confidence score from 0.0 to 1.0\n",
    "\n",
    "Strictly return your answer in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"institutions_position\": \"...\",\n",
    "  \"confidence\": ...,\n",
    "  \"explanation\": \"...\"\n",
    "}}\n",
    "\n",
    "Film Summary:\n",
    "\\\"\\\"\\\"{summary}\\\"\\\"\\\"\n",
    "\n",
    "Dialogue:\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
    "\"\"\".strip()\n",
    "\n",
    "# === API call ===\n",
    "def call_claude(prompt):\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0,\n",
    "        \"system\": \"You are a careful film analyst. Always return exactly and only the required JSON.\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 429:\n",
    "        print(\"âš ï¸ Rate limit hit. Waiting...\")\n",
    "        time.sleep(60)\n",
    "        return call_claude(prompt)\n",
    "    elif response.status_code != 200:\n",
    "        print(f\"âŒ API Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        content = response.json()[\"content\"][0][\"text\"]\n",
    "        return json.loads(content)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ JSON parsing error:\\n{response.text}\")\n",
    "        return None\n",
    "\n",
    "# === Subtitle chunking ===\n",
    "def chunk_dialogue(subs, chunk_size=5000, overlap=200):\n",
    "    text_blocks = [line.get(\"text\", \"\").strip() for line in subs if line.get(\"text\")]\n",
    "    full_text = \" \".join(text_blocks)\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(full_text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(full_text[start:end])\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# === Main loop ===\n",
    "for _, row in tqdm(matches_df.iterrows(), total=len(matches_df), desc=\"Scoring institutions_position\"):\n",
    "    filename = row[\"subtitle_filename\"]\n",
    "    json_path = subs_dir / f\"{filename}.json\"\n",
    "    summary_path = summaries_dir / f\"{filename}.srt_summary.txt\"\n",
    "\n",
    "    if not json_path.exists() or not summary_path.exists():\n",
    "        print(f\"âš ï¸ Missing files for: {filename}\")\n",
    "        continue\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        subs = json.load(f)\n",
    "\n",
    "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = f.read().strip()\n",
    "\n",
    "    chunks = chunk_dialogue(subs)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_path = output_dir / f\"{filename}_chunk{i+1}_institutions_position.json\"\n",
    "        prompt_path = prompts_dir / f\"{filename}_chunk{i+1}_institutions_position_prompt.txt\"\n",
    "\n",
    "        if output_path.exists():\n",
    "            print(f\"ðŸŸ¡ Already scored: {filename} chunk {i+1}\")\n",
    "            continue\n",
    "\n",
    "        prompt = create_institutions_position_prompt(summary, chunk)\n",
    "\n",
    "        with open(prompt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(prompt)\n",
    "\n",
    "        result = call_claude(prompt)\n",
    "        if result:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "            print(f\"âœ… Scored: {filename} chunk {i+1}\")\n",
    "        else:\n",
    "            print(f\"âŒ Failed: {filename} chunk {i+1}\")\n",
    "\n",
    "        time.sleep(1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b893c56-cf73-499a-8f1a-622ea32475f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved aggregated results to: /Users/cedricroetheli/Desktop/Benchmark/model_institutions_position_output.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# === Paths ===\n",
    "base_path = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "scored_dir = base_path / \"scored_institutions_position\"\n",
    "output_csv_path = base_path / \"model_institutions_position_output.csv\"\n",
    "\n",
    "# === Aggregate Results ===\n",
    "aggregated_results = []\n",
    "\n",
    "# Group chunk files by movie\n",
    "movie_files = defaultdict(list)\n",
    "for file in scored_dir.glob(\"*_chunk*_institutions_position.json\"):\n",
    "    movie_id = file.name.split(\"_chunk\")[0]\n",
    "    movie_files[movie_id].append(file)\n",
    "\n",
    "# Process each movie's chunks\n",
    "for movie_id, files in movie_files.items():\n",
    "    label_counts = Counter()\n",
    "    confidence_sums = defaultdict(float)\n",
    "    confidence_counts = defaultdict(int)\n",
    "    explanations = []\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                label = data.get(\"institutions_position\", \"\").strip().lower()\n",
    "                confidence = float(data.get(\"confidence\", 0.0))\n",
    "                explanation = data.get(\"explanation\", \"\")\n",
    "\n",
    "                label_counts[label] += 1\n",
    "                confidence_sums[label] += confidence\n",
    "                confidence_counts[label] += 1\n",
    "                explanations.append(f\"{label} ({confidence:.2f}): {explanation}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error in file {file.name}: {e}\")\n",
    "\n",
    "    # Compute average confidence per label\n",
    "    avg_confidences = {\n",
    "        label: round(confidence_sums[label] / confidence_counts[label], 3)\n",
    "        for label in label_counts\n",
    "    }\n",
    "\n",
    "    # Weighted vote = label with highest total confidence\n",
    "    weighted_vote = max(confidence_sums.items(), key=lambda x: x[1])[0] if confidence_sums else None\n",
    "\n",
    "    aggregated_results.append({\n",
    "        \"subtitle_filename\": movie_id,\n",
    "        \"label_counts\": dict(label_counts),\n",
    "        \"avg_confidences\": avg_confidences,\n",
    "        \"weighted_vote\": weighted_vote,\n",
    "        \"explanations\": explanations\n",
    "    })\n",
    "\n",
    "# === Save to CSV ===\n",
    "df = pd.DataFrame(aggregated_results)\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"âœ… Saved aggregated results to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56947dfa-6b37-4954-bd90-be2b7a6c5fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Evaluation complete:\n",
      "âœ… Correct: 17/20\n",
      "ðŸ“Š Accuracy: 85.00%\n",
      "ðŸ“ Saved to: /Users/cedricroetheli/Desktop/Benchmark/institutions_position_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "base = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "truth_path = base / \"benchmark_final.csv\"\n",
    "model_path = base / \"model_institutions_position_output.csv\"\n",
    "output_path = base / \"institutions_position_evaluation.csv\"\n",
    "\n",
    "# === Load Data ===\n",
    "truth_df = pd.read_csv(truth_path)\n",
    "model_df = pd.read_csv(model_path)\n",
    "\n",
    "# === Normalize filenames ===\n",
    "truth_df[\"subtitle_filename\"] = truth_df[\"subtitle_filename\"].str.strip()\n",
    "model_df[\"subtitle_filename\"] = model_df[\"subtitle_filename\"].str.strip()\n",
    "\n",
    "# === Label Mapping for Consistency ===\n",
    "label_map = {\n",
    "    \"opposed\": \"opposed\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"supported\": \"supported\"\n",
    "}\n",
    "\n",
    "def normalize_label(label):\n",
    "    if pd.isna(label):\n",
    "        return \"\"\n",
    "    label = label.lower().strip()\n",
    "    return label_map.get(label, label)\n",
    "\n",
    "def normalize_set(label_str):\n",
    "    if pd.isna(label_str):\n",
    "        return set()\n",
    "    parts = [normalize_label(part) for part in label_str.split(\"|\")]\n",
    "    return set(parts)\n",
    "\n",
    "truth_df[\"institutions_position_set\"] = truth_df[\"institutions_position\"].apply(normalize_set)\n",
    "model_df[\"normalized_vote\"] = model_df[\"weighted_vote\"].apply(normalize_label)\n",
    "\n",
    "# === Merge and Evaluate ===\n",
    "merged_df = pd.merge(model_df, truth_df, on=\"subtitle_filename\", how=\"inner\")\n",
    "merged_df[\"is_correct\"] = merged_df.apply(\n",
    "    lambda row: row[\"normalized_vote\"] in row[\"institutions_position_set\"], axis=1\n",
    ")\n",
    "\n",
    "# === Output CSV ===\n",
    "evaluation_df = merged_df[[\n",
    "    \"subtitle_filename\", \"institutions_position\", \"normalized_vote\", \"is_correct\"\n",
    "]].copy()\n",
    "evaluation_df.columns = [\"movie\", \"benchmark_institutions_position\", \"model_institutions_position\", \"is_correct\"]\n",
    "evaluation_df.to_csv(output_path, index=False)\n",
    "\n",
    "# === Summary ===\n",
    "total = len(evaluation_df)\n",
    "correct = evaluation_df[\"is_correct\"].sum()\n",
    "accuracy = correct / total if total else 0\n",
    "\n",
    "print(f\"ðŸŽ¯ Evaluation complete:\")\n",
    "print(f\"âœ… Correct: {correct}/{total}\")\n",
    "print(f\"ðŸ“Š Accuracy: {accuracy:.2%}\")\n",
    "print(f\"ðŸ“ Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416545c-3b80-49cf-b54a-8f7b075ded4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_clean_env_py310)",
   "language": "python",
   "name": "my_clean_env_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
