{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0644ca4-6ff6-4be0-84c8-09c62894882e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e2cbf1322d4ccba38c2a94e0073251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing subtitles:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "desktop = Path.home() / \"Desktop\"\n",
    "input_folder = desktop / \"Benchmark\" / \"srtraw\" # <-- Your folder with 450 SRT files\n",
    "output_folder = desktop / \"Benchmark\" / \"json_benchmark\"\n",
    "chunk_size = 30  # number of lines per chunk\n",
    "\n",
    "# === ENSURE OUTPUT FOLDER EXISTS ===\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === FUNCTION TO PARSE AND CHUNK SRT FILE ===\n",
    "def parse_srt(file_path, chunk_size=30):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    raw_blocks = re.split(r\"\\n\\s*\\n\", content.strip())\n",
    "    \n",
    "    lines = []\n",
    "    for block in raw_blocks:\n",
    "        parts = block.strip().split(\"\\n\")\n",
    "        if len(parts) >= 3:\n",
    "            text = \"\\n\".join(parts[2:])\n",
    "            lines.append(text)\n",
    "    \n",
    "    # Group into chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(lines), chunk_size):\n",
    "        chunk_text = \"\\n\".join(lines[i:i + chunk_size]).strip()\n",
    "        if chunk_text:\n",
    "            chunks.append({\n",
    "                \"chunk_id\": len(chunks) + 1,\n",
    "                \"text\": chunk_text\n",
    "            })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# === MAIN PROCESSING LOOP ===\n",
    "srt_files = list(input_folder.glob(\"*.srt\"))\n",
    "\n",
    "for srt_file in tqdm(srt_files, desc=\"Processing subtitles\"):\n",
    "    base_name = srt_file.stem\n",
    "    output_path = output_folder / f\"{base_name}.json\"\n",
    "    \n",
    "    try:\n",
    "        chunks = parse_srt(srt_file, chunk_size)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "            json.dump(chunks, out_f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to process {srt_file.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c2c4fd-3025-4a9f-9b83-b98a9d9d5d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved in your Jupyter workspace as: cleaned_filenames.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define input folder\n",
    "desktop = Path.home() / \"Desktop\"\n",
    "input_folder = desktop / \"Benchmark\" / \"srtraw\"\n",
    "\n",
    "# Use current working directory to save the CSV\n",
    "output_csv_path = Path('.') / 'cleaned_filenames.csv'\n",
    "\n",
    "# List all .srt files in the input folder\n",
    "srt_files = [f for f in os.listdir(input_folder) if f.endswith('.srt')]\n",
    "\n",
    "# Prepare data\n",
    "original_names = []\n",
    "cleaned_names = []\n",
    "\n",
    "for file in srt_files:\n",
    "    original_name = file\n",
    "    # Remove extension\n",
    "    name_without_ext = os.path.splitext(file)[0]\n",
    "    # Remove leading 4-digit year\n",
    "    name_no_year = re.sub(r'^\\d{4}', '', name_without_ext)\n",
    "    # Replace dots or underscores with spaces\n",
    "    cleaned_name = re.sub(r'[._]+', ' ', name_no_year).strip()\n",
    "\n",
    "    original_names.append(original_name)\n",
    "    cleaned_names.append(cleaned_name)\n",
    "\n",
    "# Create DataFrame and save\n",
    "df = pd.DataFrame({\n",
    "    'original_name': original_names,\n",
    "    'cleaned_name': cleaned_names\n",
    "})\n",
    "\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"CSV saved in your Jupyter workspace as: {output_csv_path.name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8866a74-90c2-432f-9f4c-e6ee407d14d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü° Summary already exists for: the Hurt Locker\n",
      "üü° Summary already exists for: the Purge\n",
      "üü° Summary already exists for: Black Panther\n",
      "üü° Summary already exists for: Night at the Museum\n",
      "üü° Summary already exists for: Rocky I\n",
      "üü° Summary already exists for: Pirates of the Caribbean the Curse of the Black Pearl\n",
      "üü° Summary already exists for: Superman\n",
      "üü° Summary already exists for: Paddington 2\n",
      "üü° Summary already exists for: Joker\n",
      "üü° Summary already exists for: the Constant Gardener\n",
      "üü° Summary already exists for: First Blood\n",
      "üü° Summary already exists for: the Hunger Games\n",
      "üü° Summary already exists for: Ghostbusters\n",
      "‚ùå No summary found for: V for Vendetta\n",
      "üü° Summary already exists for: Indiana Jones and the Raiders of the Lost Ark\n",
      "üü° Summary already exists for: Star Wars Episode Iv - A New Hope\n",
      "üü° Summary already exists for: Blood Diamond\n",
      "üü° Summary already exists for: Dont Look Up\n",
      "üü° Summary already exists for: Avatar\n",
      "üü° Summary already exists for: Back to the Future\n",
      "‚ùó Missing summaries saved to: /Users/cedricroetheli/Desktop/Benchmark/summaries/missing_summaries.txt\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Load cleaned CSV ===\n",
    "df = pd.read_csv(\"cleaned_filenames.csv\")\n",
    "\n",
    "# === Output folder for summaries ===\n",
    "summary_output_dir = Path.home() / \"Desktop\" / \"Benchmark\" / \"summaries\"\n",
    "summary_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Helper: title case with exception for \"for\", \"and\", etc. ===\n",
    "# Preserve known title exceptions exactly as needed\n",
    "def title_case(title):\n",
    "    special_cases = {\n",
    "        \"v for vendetta\": \"V for Vendetta\",\n",
    "    }\n",
    "\n",
    "    key = title.lower().strip()\n",
    "    if key in special_cases:\n",
    "        return special_cases[key]\n",
    "\n",
    "    # Default behavior: smart title casing with common word exceptions\n",
    "    words = title.split()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if word.lower() in [\"for\", \"and\", \"of\", \"the\", \"in\", \"on\", \"at\", \"to\"]:\n",
    "            result.append(word.lower())\n",
    "        else:\n",
    "            result.append(word.capitalize())\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "# === Wikipedia summary fetch function ===\n",
    "def fetch_wikipedia_summary(title, year=None):\n",
    "    from wikipedia import page, search, WikipediaException\n",
    "\n",
    "    attempts = []\n",
    "\n",
    "    # Direct forms\n",
    "    attempts.append(title)\n",
    "    if year:\n",
    "        attempts.append(f\"{title} (film)\")\n",
    "        attempts.append(f\"{title} ({year} film)\")\n",
    "    else:\n",
    "        attempts.append(f\"{title} (film)\")\n",
    "\n",
    "    # Try each attempt directly\n",
    "    for attempt in attempts:\n",
    "        try:\n",
    "            pg = page(attempt, auto_suggest=False)\n",
    "            content = pg.content\n",
    "            if \"== Plot ==\" in content:\n",
    "                start = content.find(\"== Plot ==\") + len(\"== Plot ==\")\n",
    "                end = content.find(\"==\", start)\n",
    "                return content[start:end].strip() if end != -1 else content[start:].strip()\n",
    "            else:\n",
    "                return content.split('\\n')[0]\n",
    "        except WikipediaException:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Unexpected error on '{attempt}': {e}\")\n",
    "            continue\n",
    "\n",
    "    # If all fail, try search\n",
    "    try:\n",
    "        results = search(title)\n",
    "        for result in results:\n",
    "            try:\n",
    "                pg = page(result, auto_suggest=False)\n",
    "                content = pg.content\n",
    "                if \"== Plot ==\" in content:\n",
    "                    start = content.find(\"== Plot ==\") + len(\"== Plot ==\")\n",
    "                    end = content.find(\"==\", start)\n",
    "                    return content[start:end].strip() if end != -1 else content[start:].strip()\n",
    "                else:\n",
    "                    return content.split('\\n')[0]\n",
    "            except:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Search failed for {title}: {e}\")\n",
    "\n",
    "    print(f\"‚ùå No summary found for: {title}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# === Loop through each entry and save summaries ===\n",
    "failed_titles = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    filename = row['original_name']\n",
    "    raw_title = row['cleaned_name']\n",
    "    title = title_case(raw_title)\n",
    "\n",
    "    if not isinstance(filename, str) or not filename[:4].isdigit():\n",
    "        print(f\"‚ö†Ô∏è Skipping row with bad or missing filename: {filename}\")\n",
    "        continue\n",
    "\n",
    "    year = filename[:4]\n",
    "    summary_path = summary_output_dir / f\"{filename}_summary.txt\"\n",
    "\n",
    "    if summary_path.exists():\n",
    "        print(f\"üü° Summary already exists for: {title}\")\n",
    "        continue\n",
    "\n",
    "    summary = fetch_wikipedia_summary(title, year)\n",
    "    if summary:\n",
    "        with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(summary)\n",
    "        print(f\"‚úÖ Saved summary for: {title}\")\n",
    "    else:\n",
    "        print(f\"‚ùå No summary found for: {title}\")\n",
    "        failed_titles.append(title)\n",
    "\n",
    "# === Save list of failed titles ===\n",
    "if failed_titles:\n",
    "    failed_log = summary_output_dir / \"missing_summaries.txt\"\n",
    "    with open(failed_log, \"w\", encoding=\"utf-8\") as f:\n",
    "        for t in failed_titles:\n",
    "            f.write(t + \"\\n\")\n",
    "    print(f\"‚ùó Missing summaries saved to: {failed_log}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a96824e-60fd-4ae7-9034-63fc37f5200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Manually saved summary for V for Vendetta\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "page = wikipedia.page(\"V for Vendetta (film)\", auto_suggest=False)\n",
    "content = page.content\n",
    "start = content.find(\"== Plot ==\") + len(\"== Plot ==\")\n",
    "end = content.find(\"==\", start)\n",
    "summary = content[start:end].strip() if end != -1 else content[start:].strip()\n",
    "\n",
    "with open(\"/Users/cedricroetheli/Desktop/Benchmark/summaries/2005V.for.Vendetta_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"‚úÖ Manually saved summary for V for Vendetta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3231c993-d2c2-4a11-a505-d75b5e1fa495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created matches_benchmark.csv with 20 matched entries.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "subs_dir = Path.home() / \"Desktop\" / \"Benchmark\" / \"json_benchmark\"\n",
    "summaries_dir = Path.home() / \"Desktop\" / \"Benchmark\" / \"summaries\"\n",
    "\n",
    "# List all .json subtitle files\n",
    "json_files = list(subs_dir.glob(\"*.json\"))\n",
    "\n",
    "# Build matched list\n",
    "matches = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    base_name = json_file.stem  # e.g., \"2021Dont.Look.Up\"\n",
    "    summary_file = summaries_dir / f\"{base_name}.srt_summary.txt\"\n",
    "\n",
    "    if summary_file.exists():\n",
    "        matches.append({\"subtitle_filename\": base_name})\n",
    "\n",
    "# Create DataFrame\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "# Save to CSV\n",
    "matches_df.to_csv(\"matches_benchmark.csv\", index=False)\n",
    "\n",
    "print(f\"‚úÖ Created matches_benchmark.csv with {len(matches_df)} matched entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0433cd-de56-4c70-ab07-fb5e15b07bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_clean_env_py310)",
   "language": "python",
   "name": "my_clean_env_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
