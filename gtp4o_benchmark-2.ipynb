{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4effc5ee-3e53-4d02-83bc-aeb73b57e4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):   0%|                        | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2006Blood.Diamond chunk 1\n",
      "âœ… Scored: 2006Blood.Diamond chunk 2\n",
      "âœ… Scored: 2006Blood.Diamond chunk 3\n",
      "âœ… Scored: 2006Blood.Diamond chunk 4\n",
      "âœ… Scored: 2006Blood.Diamond chunk 5\n",
      "âœ… Scored: 2006Blood.Diamond chunk 6\n",
      "âœ… Scored: 2006Blood.Diamond chunk 7\n",
      "âœ… Scored: 2006Blood.Diamond chunk 8\n",
      "âœ… Scored: 2006Blood.Diamond chunk 9\n",
      "âœ… Scored: 2006Blood.Diamond chunk 10\n",
      "âœ… Scored: 2006Blood.Diamond chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):   5%|â–Š               | 1/20 [00:36<11:28, 36.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2005The.Constant.Gardener chunk 1\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 2\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 3\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 4\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 5\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 6\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 7\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 8\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 9\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 10\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 11\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 12\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  10%|â–ˆâ–Œ              | 2/20 [01:20<12:15, 40.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 1\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 2\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 3\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 4\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 5\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  15%|â–ˆâ–ˆâ–             | 3/20 [01:40<08:55, 31.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2009Avatar chunk 1\n",
      "âœ… Scored: 2009Avatar chunk 2\n",
      "âœ… Scored: 2009Avatar chunk 3\n",
      "âœ… Scored: 2009Avatar chunk 4\n",
      "âœ… Scored: 2009Avatar chunk 5\n",
      "âœ… Scored: 2009Avatar chunk 6\n",
      "âœ… Scored: 2009Avatar chunk 7\n",
      "âœ… Scored: 2009Avatar chunk 8\n",
      "âœ… Scored: 2009Avatar chunk 9\n",
      "âœ… Scored: 2009Avatar chunk 10\n",
      "âœ… Scored: 2009Avatar chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  20%|â–ˆâ–ˆâ–ˆâ–            | 4/20 [02:20<09:17, 34.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2012The.Hunger.Games chunk 1\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 2\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 3\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 4\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 5\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 6\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 7\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  25%|â–ˆâ–ˆâ–ˆâ–ˆ            | 5/20 [02:44<07:43, 30.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1984Ghostbusters chunk 1\n",
      "âœ… Scored: 1984Ghostbusters chunk 2\n",
      "âœ… Scored: 1984Ghostbusters chunk 3\n",
      "âœ… Scored: 1984Ghostbusters chunk 4\n",
      "âœ… Scored: 1984Ghostbusters chunk 5\n",
      "âœ… Scored: 1984Ghostbusters chunk 6\n",
      "âœ… Scored: 1984Ghostbusters chunk 7\n",
      "âœ… Scored: 1984Ghostbusters chunk 8\n",
      "âœ… Scored: 1984Ghostbusters chunk 9\n",
      "âœ… Scored: 1984Ghostbusters chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 6/20 [03:27<08:07, 34.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1978Superman chunk 1\n",
      "âœ… Scored: 1978Superman chunk 2\n",
      "âœ… Scored: 1978Superman chunk 3\n",
      "âœ… Scored: 1978Superman chunk 4\n",
      "âœ… Scored: 1978Superman chunk 5\n",
      "âœ… Scored: 1978Superman chunk 6\n",
      "âœ… Scored: 1978Superman chunk 7\n",
      "âœ… Scored: 1978Superman chunk 8\n",
      "âœ… Scored: 1978Superman chunk 9\n",
      "âœ… Scored: 1978Superman chunk 10\n",
      "âœ… Scored: 1978Superman chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 7/20 [04:00<07:27, 34.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2008The.Hurt.Locker chunk 1\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 2\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 3\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 4\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 5\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 6\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 7\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 8\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 8/20 [04:28<06:27, 32.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 1\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 2\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 3\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 4\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 5\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 6\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 7\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 8\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 9\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 10\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 11\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 9/20 [05:09<06:24, 34.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 1\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 2\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 3\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 4\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 5\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 6\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 7\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 8\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 9\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 10/20 [05:41<05:42, 34.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2018Black.Panther chunk 1\n",
      "âœ… Scored: 2018Black.Panther chunk 2\n",
      "âœ… Scored: 2018Black.Panther chunk 3\n",
      "âœ… Scored: 2018Black.Panther chunk 4\n",
      "âœ… Scored: 2018Black.Panther chunk 5\n",
      "âœ… Scored: 2018Black.Panther chunk 6\n",
      "âœ… Scored: 2018Black.Panther chunk 7\n",
      "âœ… Scored: 2018Black.Panther chunk 8\n",
      "âœ… Scored: 2018Black.Panther chunk 9\n",
      "âœ… Scored: 2018Black.Panther chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 11/20 [06:14<05:03, 33.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2021Dont.Look.Up chunk 1\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 2\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 3\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 4\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 5\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 6\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 7\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 8\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 9\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 10\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 11\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 12\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 13\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 14\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 15\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 16\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 12/20 [07:13<05:31, 41.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1982First.Blood chunk 1\n",
      "âœ… Scored: 1982First.Blood chunk 2\n",
      "âœ… Scored: 1982First.Blood chunk 3\n",
      "âœ… Scored: 1982First.Blood chunk 4\n",
      "âœ… Scored: 1982First.Blood chunk 5\n",
      "âœ… Scored: 1982First.Blood chunk 6\n",
      "âœ… Scored: 1982First.Blood chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/20 [07:38<04:14, 36.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2019Joker chunk 1\n",
      "âœ… Scored: 2019Joker chunk 2\n",
      "âœ… Scored: 2019Joker chunk 3\n",
      "âœ… Scored: 2019Joker chunk 4\n",
      "âœ… Scored: 2019Joker chunk 5\n",
      "âœ… Scored: 2019Joker chunk 6\n",
      "âœ… Scored: 2019Joker chunk 7\n",
      "âœ… Scored: 2019Joker chunk 8\n",
      "âœ… Scored: 2019Joker chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/20 [08:07<03:24, 34.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2006Night.at.the.Museum chunk 1\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 2\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 3\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 4\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 5\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 6\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 7\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 8\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 9\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 15/20 [08:47<03:00, 36.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1976Rocky.I chunk 1\n",
      "âœ… Scored: 1976Rocky.I chunk 2\n",
      "âœ… Scored: 1976Rocky.I chunk 3\n",
      "âœ… Scored: 1976Rocky.I chunk 4\n",
      "âœ… Scored: 1976Rocky.I chunk 5\n",
      "âœ… Scored: 1976Rocky.I chunk 6\n",
      "âœ… Scored: 1976Rocky.I chunk 7\n",
      "âœ… Scored: 1976Rocky.I chunk 8\n",
      "âœ… Scored: 1976Rocky.I chunk 9\n",
      "âœ… Scored: 1976Rocky.I chunk 10\n",
      "âœ… Scored: 1976Rocky.I chunk 11\n",
      "âœ… Scored: 1976Rocky.I chunk 12\n",
      "âœ… Scored: 1976Rocky.I chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 16/20 [09:35<02:38, 39.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2005V.for.Vendetta chunk 1\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 2\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 3\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 4\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 5\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 6\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 7\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 8\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 9\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 10\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 11\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 12\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 13\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 14\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 15\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 17/20 [10:32<02:14, 44.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2017Paddington.2 chunk 1\n",
      "âœ… Scored: 2017Paddington.2 chunk 2\n",
      "âœ… Scored: 2017Paddington.2 chunk 3\n",
      "âœ… Scored: 2017Paddington.2 chunk 4\n",
      "âœ… Scored: 2017Paddington.2 chunk 5\n",
      "âœ… Scored: 2017Paddington.2 chunk 6\n",
      "âœ… Scored: 2017Paddington.2 chunk 7\n",
      "âœ… Scored: 2017Paddington.2 chunk 8\n",
      "âœ… Scored: 2017Paddington.2 chunk 9\n",
      "âœ… Scored: 2017Paddington.2 chunk 10\n",
      "âœ… Scored: 2017Paddington.2 chunk 11\n",
      "âœ… Scored: 2017Paddington.2 chunk 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 18/20 [11:20<01:31, 45.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1985Back.To.The.Future chunk 1\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 2\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 3\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 4\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 5\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 6\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 7\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 8\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 9\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 10\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 19/20 [12:06<00:45, 45.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2013The.Purge chunk 1\n",
      "âœ… Scored: 2013The.Purge chunk 2\n",
      "âœ… Scored: 2013The.Purge chunk 3\n",
      "âœ… Scored: 2013The.Purge chunk 4\n",
      "âœ… Scored: 2013The.Purge chunk 5\n",
      "âœ… Scored: 2013The.Purge chunk 6\n",
      "âœ… Scored: 2013The.Purge chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring hero type (OpenAI): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [12:28<00:00, 37.44s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === OpenAI API setup ===\n",
    "api_key = \"api-key\"  # ðŸ”’ Replace with your OpenAI key\n",
    "api_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# === Paths ===\n",
    "base = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "subs_dir = base / \"json_benchmark\"\n",
    "summaries_dir = base / \"summaries\"\n",
    "output_dir = base / \"scored_hero_type_openai\"\n",
    "prompts_dir = base / \"prompts_hero_type_openai\"\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "prompts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Load match file ===\n",
    "matches_df = pd.read_csv(\"matches_benchmark.csv\")\n",
    "\n",
    "# === Prompt template ===\n",
    "def create_hero_type_prompt(summary, chunk):\n",
    "    return f\"\"\"\n",
    "You are analyzing the protagonist(s) of a film based on its subtitles and plot summary.\n",
    "\n",
    "Your task is to classify the central figure(s) in the film into one of six predefined hero type categories.\n",
    "\n",
    "The six hero type categories are defined as follows:\n",
    "\n",
    "- Ordinary Individual: A regular person caught in extraordinary circumstances; succeeds through grit, not destiny.\n",
    "- Heroic Individual: A character with clear heroic traitsâ€”bravery, leadership, strength, often iconic or idealized.\n",
    "- Group / Ensemble: A collective effort drives the story; no single person is the exclusive hero.\n",
    "- Institution / System: An organization or societal body serves as protagonist or moral force.\n",
    "- None: There is no clear hero in the film.\n",
    "- Anti-Hero: The main character may be active but is morally ambiguous, selfish, or destructive.\n",
    "\n",
    "Please:\n",
    "1. Select the most appropriate hero type label\n",
    "2. Provide a brief explanation (1â€“2 sentences max)\n",
    "3. Include a confidence score from 0.0 to 1.0\n",
    "\n",
    "Strictly return your answer in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"hero_type\": \"...\",\n",
    "  \"confidence\": ...,\n",
    "  \"explanation\": \"...\"\n",
    "}}\n",
    "\n",
    "Film Summary:\n",
    "\\\"\\\"\\\"{summary}\\\"\\\"\\\"\n",
    "\n",
    "Dialogue:\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
    "\"\"\".strip()\n",
    "\n",
    "# === OpenAI API call ===\n",
    "def call_openai(prompt):\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 500,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a careful film analyst. Always return exactly and only the required JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 429:\n",
    "        print(\"âš ï¸ Rate limit hit. Waiting...\")\n",
    "        time.sleep(60)\n",
    "        return call_openai(prompt)\n",
    "    elif response.status_code != 200:\n",
    "        print(f\"âŒ API Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # Strip code fences if present\n",
    "        if content.strip().startswith(\"```\"):\n",
    "            content = content.strip().strip(\"`\")  # removes leading/trailing ```\n",
    "            if content.lower().startswith(\"json\"):\n",
    "                content = content[4:].strip()  # remove 'json\\n'\n",
    "\n",
    "        return json.loads(content)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ JSON parsing error:\\n{response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# === Subtitle chunking ===\n",
    "def chunk_dialogue(subs, chunk_size=5000, overlap=200):\n",
    "    text_blocks = [line.get(\"text\", \"\").strip() for line in subs if line.get(\"text\")]\n",
    "    full_text = \" \".join(text_blocks)\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(full_text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(full_text[start:end])\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# === Main loop ===\n",
    "for _, row in tqdm(matches_df.iterrows(), total=len(matches_df), desc=\"Scoring hero type (OpenAI)\"):\n",
    "    filename = row[\"subtitle_filename\"]\n",
    "    json_path = subs_dir / f\"{filename}.json\"\n",
    "    summary_path = summaries_dir / f\"{filename}.srt_summary.txt\"\n",
    "\n",
    "    if not json_path.exists() or not summary_path.exists():\n",
    "        print(f\"âš ï¸ Missing files for: {filename}\")\n",
    "        continue\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        subs = json.load(f)\n",
    "\n",
    "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = f.read().strip()\n",
    "\n",
    "    chunks = chunk_dialogue(subs)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_path = output_dir / f\"{filename}_chunk{i+1}_hero_type.json\"\n",
    "        prompt_path = prompts_dir / f\"{filename}_chunk{i+1}_hero_type_prompt.txt\"\n",
    "\n",
    "        if output_path.exists():\n",
    "            print(f\"ðŸŸ¡ Already scored: {filename} chunk {i+1}\")\n",
    "            continue\n",
    "\n",
    "        prompt = create_hero_type_prompt(summary, chunk)\n",
    "\n",
    "        with open(prompt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(prompt)\n",
    "\n",
    "        result = call_openai(prompt)\n",
    "        if result:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "            print(f\"âœ… Scored: {filename} chunk {i+1}\")\n",
    "        else:\n",
    "            print(f\"âŒ Failed: {filename} chunk {i+1}\")\n",
    "\n",
    "        time.sleep(1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e0938f-6bca-4e17-865e-d2f7b0657b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved aggregated results to: /Users/cedricroetheli/Desktop/Benchmark/model_hero_type_output_openai.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# === Paths ===\n",
    "base_path = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "scored_dir = base_path / \"scored_hero_type_openai\"\n",
    "output_csv_path = base_path / \"model_hero_type_output_openai.csv\"\n",
    "\n",
    "# === Aggregate Results ===\n",
    "aggregated_results = []\n",
    "\n",
    "# Group chunk files by movie\n",
    "movie_files = defaultdict(list)\n",
    "for file in scored_dir.glob(\"*_chunk*_hero_type.json\"):\n",
    "    movie_id = file.name.split(\"_chunk\")[0]\n",
    "    movie_files[movie_id].append(file)\n",
    "\n",
    "# Process each movie's chunks\n",
    "for movie_id, files in movie_files.items():\n",
    "    label_counts = Counter()\n",
    "    confidence_sums = defaultdict(float)\n",
    "    confidence_counts = defaultdict(int)\n",
    "    explanations = []\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                label = data.get(\"hero_type\", \"\").strip().lower()\n",
    "                confidence = float(data.get(\"confidence\", 0.0))\n",
    "                explanation = data.get(\"explanation\", \"\")\n",
    "\n",
    "                label_counts[label] += 1\n",
    "                confidence_sums[label] += confidence\n",
    "                confidence_counts[label] += 1\n",
    "                explanations.append(f\"{label} ({confidence:.2f}): {explanation}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error in file {file.name}: {e}\")\n",
    "\n",
    "    # Compute average confidence per label\n",
    "    avg_confidences = {\n",
    "        label: round(confidence_sums[label] / confidence_counts[label], 3)\n",
    "        for label in label_counts\n",
    "    }\n",
    "\n",
    "    # Weighted vote = label with highest total confidence\n",
    "    weighted_vote = max(confidence_sums.items(), key=lambda x: x[1])[0] if confidence_sums else None\n",
    "\n",
    "    aggregated_results.append({\n",
    "        \"subtitle_filename\": movie_id,\n",
    "        \"label_counts\": dict(label_counts),\n",
    "        \"avg_confidences\": avg_confidences,\n",
    "        \"weighted_vote\": weighted_vote,\n",
    "        \"explanations\": explanations\n",
    "    })\n",
    "\n",
    "# === Save to CSV ===\n",
    "df = pd.DataFrame(aggregated_results)\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"âœ… Saved aggregated results to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea5df6a-4e9d-437d-974b-8aea63a7e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Evaluation complete:\n",
      "âœ… Correct: 15/20\n",
      "ðŸ“Š Accuracy: 75.00%\n",
      "ðŸ“ Saved to: /Users/cedricroetheli/Desktop/Benchmark/hero_type_evaluation_openai.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "base = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "truth_path = base / \"benchmark_final.csv\"\n",
    "model_path = base / \"model_hero_type_output_openai.csv\"\n",
    "output_path = base / \"hero_type_evaluation_openai.csv\"\n",
    "\n",
    "# === Load Data ===\n",
    "truth_df = pd.read_csv(truth_path)\n",
    "model_df = pd.read_csv(model_path)\n",
    "\n",
    "# === Normalize filenames for matching ===\n",
    "truth_df[\"subtitle_filename\"] = truth_df[\"subtitle_filename\"].str.strip()\n",
    "model_df[\"subtitle_filename\"] = model_df[\"subtitle_filename\"].str.strip()\n",
    "\n",
    "# === Define Label Normalization ===\n",
    "label_map = {\n",
    "    \"heroic individual\": \"heroic\",\n",
    "    \"ordinary individual\": \"ordinary\",\n",
    "    \"group / ensemble\": \"group\",\n",
    "    \"institution / system\": \"institution\",\n",
    "    \"none\": \"none\",\n",
    "    \"anti-heroic\": \"anti-hero\",\n",
    "    \"none / anti-heroic\": \"none|anti-hero\",\n",
    "    \"none|anti-heroic\": \"none|anti-hero\",\n",
    "    \"anti-hero\": \"anti-hero\",\n",
    "    \"none / anti-hero\": \"none|anti-hero\",\n",
    "    \"group\": \"group\",\n",
    "    \"heroic\": \"heroic\",\n",
    "    \"ordinary\": \"ordinary\"\n",
    "}\n",
    "\n",
    "# Apply label map to both sides\n",
    "def normalize_label(label):\n",
    "    if pd.isna(label):\n",
    "        return \"\"\n",
    "    label = label.lower().strip()\n",
    "    return label_map.get(label, label)\n",
    "\n",
    "# Normalize benchmark labels (which may have multiple correct ones)\n",
    "def normalize_set(label_str):\n",
    "    if pd.isna(label_str):\n",
    "        return set()\n",
    "    parts = [normalize_label(part) for part in label_str.split(\"|\")]\n",
    "    return set(parts)\n",
    "\n",
    "truth_df[\"hero_type_set\"] = truth_df[\"hero_type\"].apply(normalize_set)\n",
    "model_df[\"normalized_vote\"] = model_df[\"weighted_vote\"].apply(normalize_label)\n",
    "\n",
    "# === Merge and Evaluate ===\n",
    "merged_df = pd.merge(model_df, truth_df, on=\"subtitle_filename\", how=\"inner\")\n",
    "merged_df[\"is_correct\"] = merged_df.apply(\n",
    "    lambda row: row[\"normalized_vote\"] in row[\"hero_type_set\"], axis=1\n",
    ")\n",
    "\n",
    "# === Prepare Output CSV ===\n",
    "evaluation_df = merged_df[[\n",
    "    \"subtitle_filename\", \"hero_type\", \"normalized_vote\", \"is_correct\"\n",
    "]].copy()\n",
    "evaluation_df.columns = [\"movie\", \"benchmark_hero_type\", \"model_hero_type\", \"is_correct\"]\n",
    "evaluation_df.to_csv(output_path, index=False)\n",
    "\n",
    "# === Summary ===\n",
    "total = len(evaluation_df)\n",
    "correct = evaluation_df[\"is_correct\"].sum()\n",
    "accuracy = correct / total if total else 0\n",
    "\n",
    "print(f\"ðŸŽ¯ Evaluation complete:\")\n",
    "print(f\"âœ… Correct: {correct}/{total}\")\n",
    "print(f\"ðŸ“Š Accuracy: {accuracy:.2%}\")\n",
    "print(f\"ðŸ“ Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68b7935a-8ec2-4c26-a4d3-1d9e5da923d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):   0%|            | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2006Blood.Diamond chunk 1\n",
      "âœ… Scored: 2006Blood.Diamond chunk 2\n",
      "âœ… Scored: 2006Blood.Diamond chunk 3\n",
      "âœ… Scored: 2006Blood.Diamond chunk 4\n",
      "âœ… Scored: 2006Blood.Diamond chunk 5\n",
      "âœ… Scored: 2006Blood.Diamond chunk 6\n",
      "âœ… Scored: 2006Blood.Diamond chunk 7\n",
      "âœ… Scored: 2006Blood.Diamond chunk 8\n",
      "âœ… Scored: 2006Blood.Diamond chunk 9\n",
      "âœ… Scored: 2006Blood.Diamond chunk 10\n",
      "âœ… Scored: 2006Blood.Diamond chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):   5%|â–   | 1/20 [00:40<12:53, 40.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2005The.Constant.Gardener chunk 1\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 2\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 3\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 4\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 5\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 6\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 7\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 8\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 9\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 10\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 11\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 12\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  10%|â–   | 2/20 [01:19<11:49, 39.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 1\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 2\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 3\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 4\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 5\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  15%|â–Œ   | 3/20 [01:39<08:38, 30.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2009Avatar chunk 1\n",
      "âœ… Scored: 2009Avatar chunk 2\n",
      "âœ… Scored: 2009Avatar chunk 3\n",
      "âœ… Scored: 2009Avatar chunk 4\n",
      "âœ… Scored: 2009Avatar chunk 5\n",
      "âœ… Scored: 2009Avatar chunk 6\n",
      "âœ… Scored: 2009Avatar chunk 7\n",
      "âœ… Scored: 2009Avatar chunk 8\n",
      "âœ… Scored: 2009Avatar chunk 9\n",
      "âœ… Scored: 2009Avatar chunk 10\n",
      "âœ… Scored: 2009Avatar chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  20%|â–Š   | 4/20 [02:14<08:38, 32.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2012The.Hunger.Games chunk 1\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 2\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 3\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 4\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 5\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 6\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 7\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  25%|â–ˆ   | 5/20 [02:42<07:42, 30.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1984Ghostbusters chunk 1\n",
      "âœ… Scored: 1984Ghostbusters chunk 2\n",
      "âœ… Scored: 1984Ghostbusters chunk 3\n",
      "âœ… Scored: 1984Ghostbusters chunk 4\n",
      "âœ… Scored: 1984Ghostbusters chunk 5\n",
      "âœ… Scored: 1984Ghostbusters chunk 6\n",
      "âœ… Scored: 1984Ghostbusters chunk 7\n",
      "âœ… Scored: 1984Ghostbusters chunk 8\n",
      "âœ… Scored: 1984Ghostbusters chunk 9\n",
      "âœ… Scored: 1984Ghostbusters chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  30%|â–ˆâ–  | 6/20 [03:14<07:19, 31.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1978Superman chunk 1\n",
      "âœ… Scored: 1978Superman chunk 2\n",
      "âœ… Scored: 1978Superman chunk 3\n",
      "âœ… Scored: 1978Superman chunk 4\n",
      "âœ… Scored: 1978Superman chunk 5\n",
      "âœ… Scored: 1978Superman chunk 6\n",
      "âœ… Scored: 1978Superman chunk 7\n",
      "âœ… Scored: 1978Superman chunk 8\n",
      "âœ… Scored: 1978Superman chunk 9\n",
      "âœ… Scored: 1978Superman chunk 10\n",
      "âœ… Scored: 1978Superman chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  35%|â–ˆâ–  | 7/20 [03:50<07:05, 32.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2008The.Hurt.Locker chunk 1\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 2\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 3\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 4\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 5\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 6\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 7\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 8\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  40%|â–ˆâ–Œ  | 8/20 [04:18<06:16, 31.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 1\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 2\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 3\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 4\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 5\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 6\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 7\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 8\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 9\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 10\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 11\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  45%|â–ˆâ–Š  | 9/20 [04:55<06:03, 33.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 1\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 2\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 3\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 4\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 5\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 6\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 7\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 8\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 9\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  50%|â–ˆâ–Œ | 10/20 [05:26<05:23, 32.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2018Black.Panther chunk 1\n",
      "âœ… Scored: 2018Black.Panther chunk 2\n",
      "âœ… Scored: 2018Black.Panther chunk 3\n",
      "âœ… Scored: 2018Black.Panther chunk 4\n",
      "âœ… Scored: 2018Black.Panther chunk 5\n",
      "âœ… Scored: 2018Black.Panther chunk 6\n",
      "âœ… Scored: 2018Black.Panther chunk 7\n",
      "âœ… Scored: 2018Black.Panther chunk 8\n",
      "âœ… Scored: 2018Black.Panther chunk 9\n",
      "âœ… Scored: 2018Black.Panther chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  55%|â–ˆâ–‹ | 11/20 [05:55<04:42, 31.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2021Dont.Look.Up chunk 1\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 2\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 3\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 4\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 5\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 6\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 7\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 8\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 9\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 10\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 11\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 12\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 13\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 14\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 15\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 16\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  60%|â–ˆâ–Š | 12/20 [06:45<04:57, 37.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1982First.Blood chunk 1\n",
      "âœ… Scored: 1982First.Blood chunk 2\n",
      "âœ… Scored: 1982First.Blood chunk 3\n",
      "âœ… Scored: 1982First.Blood chunk 4\n",
      "âœ… Scored: 1982First.Blood chunk 5\n",
      "âœ… Scored: 1982First.Blood chunk 6\n",
      "âœ… Scored: 1982First.Blood chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  65%|â–ˆâ–‰ | 13/20 [07:07<03:46, 32.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2019Joker chunk 1\n",
      "âœ… Scored: 2019Joker chunk 2\n",
      "âœ… Scored: 2019Joker chunk 3\n",
      "âœ… Scored: 2019Joker chunk 4\n",
      "âœ… Scored: 2019Joker chunk 5\n",
      "âœ… Scored: 2019Joker chunk 6\n",
      "âœ… Scored: 2019Joker chunk 7\n",
      "âœ… Scored: 2019Joker chunk 8\n",
      "âœ… Scored: 2019Joker chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  70%|â–ˆâ–ˆ | 14/20 [07:36<03:08, 31.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2006Night.at.the.Museum chunk 1\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 2\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 3\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 4\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 5\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 6\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 7\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 8\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 9\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  75%|â–ˆâ–ˆâ–Ž| 15/20 [08:11<02:41, 32.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1976Rocky.I chunk 1\n",
      "âœ… Scored: 1976Rocky.I chunk 2\n",
      "âœ… Scored: 1976Rocky.I chunk 3\n",
      "âœ… Scored: 1976Rocky.I chunk 4\n",
      "âœ… Scored: 1976Rocky.I chunk 5\n",
      "âœ… Scored: 1976Rocky.I chunk 6\n",
      "âœ… Scored: 1976Rocky.I chunk 7\n",
      "âœ… Scored: 1976Rocky.I chunk 8\n",
      "âœ… Scored: 1976Rocky.I chunk 9\n",
      "âœ… Scored: 1976Rocky.I chunk 10\n",
      "âœ… Scored: 1976Rocky.I chunk 11\n",
      "âœ… Scored: 1976Rocky.I chunk 12\n",
      "âœ… Scored: 1976Rocky.I chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  80%|â–ˆâ–ˆâ–| 16/20 [08:53<02:22, 35.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2005V.for.Vendetta chunk 1\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 2\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 3\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 4\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 5\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 6\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 7\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 8\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 9\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 10\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 11\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 12\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 13\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 14\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 15\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  85%|â–ˆâ–ˆâ–Œ| 17/20 [09:55<02:10, 43.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2017Paddington.2 chunk 1\n",
      "âœ… Scored: 2017Paddington.2 chunk 2\n",
      "âœ… Scored: 2017Paddington.2 chunk 3\n",
      "âœ… Scored: 2017Paddington.2 chunk 4\n",
      "âœ… Scored: 2017Paddington.2 chunk 5\n",
      "âœ… Scored: 2017Paddington.2 chunk 6\n",
      "âœ… Scored: 2017Paddington.2 chunk 7\n",
      "âœ… Scored: 2017Paddington.2 chunk 8\n",
      "âœ… Scored: 2017Paddington.2 chunk 9\n",
      "âœ… Scored: 2017Paddington.2 chunk 10\n",
      "âœ… Scored: 2017Paddington.2 chunk 11\n",
      "âœ… Scored: 2017Paddington.2 chunk 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  90%|â–ˆâ–ˆâ–‹| 18/20 [10:34<01:24, 42.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1985Back.To.The.Future chunk 1\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 2\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 3\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 4\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 5\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 6\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 7\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 8\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 9\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 10\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI):  95%|â–ˆâ–ˆâ–Š| 19/20 [11:09<00:40, 40.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2013The.Purge chunk 1\n",
      "âœ… Scored: 2013The.Purge chunk 2\n",
      "âœ… Scored: 2013The.Purge chunk 3\n",
      "âœ… Scored: 2013The.Purge chunk 4\n",
      "âœ… Scored: 2013The.Purge chunk 5\n",
      "âœ… Scored: 2013The.Purge chunk 6\n",
      "âœ… Scored: 2013The.Purge chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring institutions_position (OpenAI): 100%|â–ˆâ–ˆâ–ˆ| 20/20 [11:31<00:00, 34.55s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === OpenAI API setup ===\n",
    "api_key = \"api-key\"  # ðŸ” Replace this with your OpenAI key\n",
    "api_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# === Paths ===\n",
    "base = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "subs_dir = base / \"json_benchmark\"\n",
    "summaries_dir = base / \"summaries\"\n",
    "output_dir = base / \"scored_institutions_position_openai\"\n",
    "prompts_dir = base / \"prompts_institutions_position_openai\"\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "prompts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "matches_df = pd.read_csv(base / \"matches_benchmark.csv\")\n",
    "\n",
    "# === Prompt template ===\n",
    "def create_institutions_position_prompt(summary, chunk):\n",
    "    return f\"\"\"\n",
    "You are analyzing how a film portrays bureaucracy, the state, and other formal institutions.\n",
    "\n",
    "Your task is to classify the filmâ€™s overall stance toward these institutions into one of three categories:\n",
    "\n",
    "- Opposed: Institutions are portrayed as corrupt, repressive, harmful, or adversarial. The narrative or characters resist or challenge them.\n",
    "- Neutral: Institutions are present but play a limited, mixed, or ambiguous role. They are neither clearly positive nor negative.\n",
    "- Supported: Institutions are shown as legitimate, effective, or beneficial. The story affirms their role or value.\n",
    "\n",
    "Please:\n",
    "1. Select one of the three stance labels: \"opposed\", \"neutral\", or \"supported\"\n",
    "2. Provide a brief explanation (1â€“2 sentences max)\n",
    "3. Include a confidence score from 0.0 to 1.0\n",
    "\n",
    "Strictly return your answer in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"institutions_position\": \"...\",\n",
    "  \"confidence\": ...,\n",
    "  \"explanation\": \"...\"\n",
    "}}\n",
    "\n",
    "Film Summary:\n",
    "\\\"\\\"\\\"{summary}\\\"\\\"\\\"\n",
    "\n",
    "Dialogue:\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
    "\"\"\".strip()\n",
    "\n",
    "# === OpenAI API call ===\n",
    "def call_openai(prompt):\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 500,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a careful film analyst. Always return exactly and only the required JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 429:\n",
    "        print(\"âš ï¸ Rate limit hit. Waiting 60s...\")\n",
    "        time.sleep(60)\n",
    "        return call_openai(prompt)\n",
    "    elif response.status_code != 200:\n",
    "        print(f\"âŒ API Error: {response.status_code}\\n{response.text}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        json_str = content.strip().strip(\"```json\").strip(\"```\")\n",
    "        return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ JSON parsing error:\\n{response.text}\")\n",
    "        return None\n",
    "\n",
    "# === Chunk subtitles ===\n",
    "def chunk_dialogue(subs, chunk_size=5000, overlap=200):\n",
    "    text_blocks = [line.get(\"text\", \"\").strip() for line in subs if line.get(\"text\")]\n",
    "    full_text = \" \".join(text_blocks)\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(full_text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(full_text[start:end])\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# === Main loop ===\n",
    "for _, row in tqdm(matches_df.iterrows(), total=len(matches_df), desc=\"Scoring institutions_position (OpenAI)\"):\n",
    "    filename = row[\"subtitle_filename\"]\n",
    "    json_path = subs_dir / f\"{filename}.json\"\n",
    "    summary_path = summaries_dir / f\"{filename}.srt_summary.txt\"\n",
    "\n",
    "    if not json_path.exists() or not summary_path.exists():\n",
    "        print(f\"âš ï¸ Missing files for: {filename}\")\n",
    "        continue\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        subs = json.load(f)\n",
    "\n",
    "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = f.read().strip()\n",
    "\n",
    "    chunks = chunk_dialogue(subs)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_path = output_dir / f\"{filename}_chunk{i+1}_institutions_position.json\"\n",
    "        prompt_path = prompts_dir / f\"{filename}_chunk{i+1}_institutions_position_prompt.txt\"\n",
    "\n",
    "        if output_path.exists():\n",
    "            print(f\"ðŸŸ¡ Already scored: {filename} chunk {i+1}\")\n",
    "            continue\n",
    "\n",
    "        prompt = create_institutions_position_prompt(summary, chunk)\n",
    "\n",
    "        with open(prompt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(prompt)\n",
    "\n",
    "        result = call_openai(prompt)\n",
    "        if result:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "            print(f\"âœ… Scored: {filename} chunk {i+1}\")\n",
    "        else:\n",
    "            print(f\"âŒ Failed: {filename} chunk {i+1}\")\n",
    "\n",
    "        time.sleep(1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a378a1bd-f0de-4781-a25d-6dfe8c3744c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved aggregated results to: /Users/cedricroetheli/Desktop/Benchmark/model_institutions_position_output_openai.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# === Paths ===\n",
    "base_path = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "scored_dir = base_path / \"scored_institutions_position_openai\"  # ðŸ’¡ Adjusted directory\n",
    "output_csv_path = base_path / \"model_institutions_position_output_openai.csv\"  # ðŸ’¾ New output file\n",
    "\n",
    "# === Aggregate Results ===\n",
    "aggregated_results = []\n",
    "\n",
    "# Group chunk files by movie\n",
    "movie_files = defaultdict(list)\n",
    "for file in scored_dir.glob(\"*_chunk*_institutions_position.json\"):\n",
    "    movie_id = file.name.split(\"_chunk\")[0]\n",
    "    movie_files[movie_id].append(file)\n",
    "\n",
    "# Process each movie's chunks\n",
    "for movie_id, files in movie_files.items():\n",
    "    label_counts = Counter()\n",
    "    confidence_sums = defaultdict(float)\n",
    "    confidence_counts = defaultdict(int)\n",
    "    explanations = []\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                label = data.get(\"institutions_position\", \"\").strip().lower()\n",
    "                confidence = float(data.get(\"confidence\", 0.0))\n",
    "                explanation = data.get(\"explanation\", \"\")\n",
    "\n",
    "                label_counts[label] += 1\n",
    "                confidence_sums[label] += confidence\n",
    "                confidence_counts[label] += 1\n",
    "                explanations.append(f\"{label} ({confidence:.2f}): {explanation}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error in file {file.name}: {e}\")\n",
    "\n",
    "    # Compute average confidence per label\n",
    "    avg_confidences = {\n",
    "        label: round(confidence_sums[label] / confidence_counts[label], 3)\n",
    "        for label in label_counts\n",
    "    }\n",
    "\n",
    "    # Weighted vote = label with highest total confidence\n",
    "    weighted_vote = max(confidence_sums.items(), key=lambda x: x[1])[0] if confidence_sums else None\n",
    "\n",
    "    aggregated_results.append({\n",
    "        \"subtitle_filename\": movie_id,\n",
    "        \"label_counts\": dict(label_counts),\n",
    "        \"avg_confidences\": avg_confidences,\n",
    "        \"weighted_vote\": weighted_vote,\n",
    "        \"explanations\": explanations\n",
    "    })\n",
    "\n",
    "# === Save to CSV ===\n",
    "df = pd.DataFrame(aggregated_results)\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"âœ… Saved aggregated results to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ba1746c-d75e-4b35-879b-030c85a2e32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Evaluation complete:\n",
      "âœ… Correct: 18/20\n",
      "ðŸ“Š Accuracy: 90.00%\n",
      "ðŸ“ Saved to: /Users/cedricroetheli/Desktop/Benchmark/institutions_position_evaluation_openai.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "base = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "truth_path = base / \"benchmark_final.csv\"\n",
    "model_path = base / \"model_institutions_position_output_openai.csv\"  # ðŸ’¡ Adjusted for OpenAI output\n",
    "output_path = base / \"institutions_position_evaluation_openai.csv\"   # ðŸ’¡ New evaluation output\n",
    "\n",
    "# === Load Data ===\n",
    "truth_df = pd.read_csv(truth_path)\n",
    "model_df = pd.read_csv(model_path)\n",
    "\n",
    "# === Normalize filenames ===\n",
    "truth_df[\"subtitle_filename\"] = truth_df[\"subtitle_filename\"].str.strip()\n",
    "model_df[\"subtitle_filename\"] = model_df[\"subtitle_filename\"].str.strip()\n",
    "\n",
    "# === Label Mapping for Consistency ===\n",
    "label_map = {\n",
    "    \"opposed\": \"opposed\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"supported\": \"supported\"\n",
    "}\n",
    "\n",
    "def normalize_label(label):\n",
    "    if pd.isna(label):\n",
    "        return \"\"\n",
    "    label = label.lower().strip()\n",
    "    return label_map.get(label, label)\n",
    "\n",
    "def normalize_set(label_str):\n",
    "    if pd.isna(label_str):\n",
    "        return set()\n",
    "    parts = [normalize_label(part) for part in label_str.split(\"|\")]\n",
    "    return set(parts)\n",
    "\n",
    "# Apply normalization\n",
    "truth_df[\"institutions_position_set\"] = truth_df[\"institutions_position\"].apply(normalize_set)\n",
    "model_df[\"normalized_vote\"] = model_df[\"weighted_vote\"].apply(normalize_label)\n",
    "\n",
    "# === Merge and Evaluate ===\n",
    "merged_df = pd.merge(model_df, truth_df, on=\"subtitle_filename\", how=\"inner\")\n",
    "merged_df[\"is_correct\"] = merged_df.apply(\n",
    "    lambda row: row[\"normalized_vote\"] in row[\"institutions_position_set\"], axis=1\n",
    ")\n",
    "\n",
    "# === Output CSV ===\n",
    "evaluation_df = merged_df[[\n",
    "    \"subtitle_filename\", \"institutions_position\", \"normalized_vote\", \"is_correct\"\n",
    "]].copy()\n",
    "evaluation_df.columns = [\"movie\", \"benchmark_institutions_position\", \"model_institutions_position\", \"is_correct\"]\n",
    "evaluation_df.to_csv(output_path, index=False)\n",
    "\n",
    "# === Summary ===\n",
    "total = len(evaluation_df)\n",
    "correct = evaluation_df[\"is_correct\"].sum()\n",
    "accuracy = correct / total if total else 0\n",
    "\n",
    "print(f\"ðŸŽ¯ Evaluation complete:\")\n",
    "print(f\"âœ… Correct: {correct}/{total}\")\n",
    "print(f\"ðŸ“Š Accuracy: {accuracy:.2%}\")\n",
    "print(f\"ðŸ“ Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5dc29-3497-4c82-84d8-664a2ec7028f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d260365e-b13b-4f12-973d-9cb35906822a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):   0%|                     | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2006Blood.Diamond chunk 1\n",
      "âœ… Scored: 2006Blood.Diamond chunk 2\n",
      "âœ… Scored: 2006Blood.Diamond chunk 3\n",
      "âœ… Scored: 2006Blood.Diamond chunk 4\n",
      "âœ… Scored: 2006Blood.Diamond chunk 5\n",
      "âœ… Scored: 2006Blood.Diamond chunk 6\n",
      "âœ… Scored: 2006Blood.Diamond chunk 7\n",
      "âœ… Scored: 2006Blood.Diamond chunk 8\n",
      "âœ… Scored: 2006Blood.Diamond chunk 9\n",
      "âœ… Scored: 2006Blood.Diamond chunk 10\n",
      "âœ… Scored: 2006Blood.Diamond chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):   5%|â–‹            | 1/20 [00:41<13:17, 41.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2005The.Constant.Gardener chunk 1\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 2\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 3\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 4\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 5\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 6\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 7\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 8\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 9\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 10\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 11\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 12\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  10%|â–ˆâ–Ž           | 2/20 [01:21<12:13, 40.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 1\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 2\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 3\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 4\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 5\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  15%|â–ˆâ–‰           | 3/20 [01:42<08:56, 31.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2009Avatar chunk 1\n",
      "âœ… Scored: 2009Avatar chunk 2\n",
      "âœ… Scored: 2009Avatar chunk 3\n",
      "âœ… Scored: 2009Avatar chunk 4\n",
      "âœ… Scored: 2009Avatar chunk 5\n",
      "âœ… Scored: 2009Avatar chunk 6\n",
      "âœ… Scored: 2009Avatar chunk 7\n",
      "âœ… Scored: 2009Avatar chunk 8\n",
      "âœ… Scored: 2009Avatar chunk 9\n",
      "âœ… Scored: 2009Avatar chunk 10\n",
      "âœ… Scored: 2009Avatar chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  20%|â–ˆâ–ˆâ–Œ          | 4/20 [02:19<08:57, 33.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2012The.Hunger.Games chunk 1\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 2\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 3\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 4\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 5\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 6\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 7\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  25%|â–ˆâ–ˆâ–ˆâ–Ž         | 5/20 [02:45<07:44, 31.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1984Ghostbusters chunk 1\n",
      "âœ… Scored: 1984Ghostbusters chunk 2\n",
      "âœ… Scored: 1984Ghostbusters chunk 3\n",
      "âœ… Scored: 1984Ghostbusters chunk 4\n",
      "âœ… Scored: 1984Ghostbusters chunk 5\n",
      "âœ… Scored: 1984Ghostbusters chunk 6\n",
      "âœ… Scored: 1984Ghostbusters chunk 7\n",
      "âœ… Scored: 1984Ghostbusters chunk 8\n",
      "âœ… Scored: 1984Ghostbusters chunk 9\n",
      "âœ… Scored: 1984Ghostbusters chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  30%|â–ˆâ–ˆâ–ˆâ–‰         | 6/20 [03:16<07:15, 31.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1978Superman chunk 1\n",
      "âœ… Scored: 1978Superman chunk 2\n",
      "âœ… Scored: 1978Superman chunk 3\n",
      "âœ… Scored: 1978Superman chunk 4\n",
      "âœ… Scored: 1978Superman chunk 5\n",
      "âœ… Scored: 1978Superman chunk 6\n",
      "âœ… Scored: 1978Superman chunk 7\n",
      "âœ… Scored: 1978Superman chunk 8\n",
      "âœ… Scored: 1978Superman chunk 9\n",
      "âœ… Scored: 1978Superman chunk 10\n",
      "âœ… Scored: 1978Superman chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 7/20 [03:51<06:59, 32.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2008The.Hurt.Locker chunk 1\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 2\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 3\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 4\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 5\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 6\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 7\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 8\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 8/20 [04:18<06:08, 30.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 1\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 2\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 3\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 4\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 5\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 6\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 7\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 8\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 9\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 10\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 11\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 9/20 [04:59<06:11, 33.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 1\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 2\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 3\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 4\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 5\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 6\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 7\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 8\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 9\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 10/20 [05:36<05:47, 34.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2018Black.Panther chunk 1\n",
      "âœ… Scored: 2018Black.Panther chunk 2\n",
      "âœ… Scored: 2018Black.Panther chunk 3\n",
      "âœ… Scored: 2018Black.Panther chunk 4\n",
      "âœ… Scored: 2018Black.Panther chunk 5\n",
      "âœ… Scored: 2018Black.Panther chunk 6\n",
      "âœ… Scored: 2018Black.Panther chunk 7\n",
      "âœ… Scored: 2018Black.Panther chunk 8\n",
      "âœ… Scored: 2018Black.Panther chunk 9\n",
      "âœ… Scored: 2018Black.Panther chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 11/20 [06:08<05:06, 34.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2021Dont.Look.Up chunk 1\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 2\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 3\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 4\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 5\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 6\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 7\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 8\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 9\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 10\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 11\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 12\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 13\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 14\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 15\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 16\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/20 [07:05<05:28, 41.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1982First.Blood chunk 1\n",
      "âœ… Scored: 1982First.Blood chunk 2\n",
      "âœ… Scored: 1982First.Blood chunk 3\n",
      "âœ… Scored: 1982First.Blood chunk 4\n",
      "âœ… Scored: 1982First.Blood chunk 5\n",
      "âœ… Scored: 1982First.Blood chunk 6\n",
      "âœ… Scored: 1982First.Blood chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 13/20 [07:28<04:07, 35.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2019Joker chunk 1\n",
      "âœ… Scored: 2019Joker chunk 2\n",
      "âœ… Scored: 2019Joker chunk 3\n",
      "âœ… Scored: 2019Joker chunk 4\n",
      "âœ… Scored: 2019Joker chunk 5\n",
      "âœ… Scored: 2019Joker chunk 6\n",
      "âœ… Scored: 2019Joker chunk 7\n",
      "âœ… Scored: 2019Joker chunk 8\n",
      "âœ… Scored: 2019Joker chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14/20 [07:56<03:19, 33.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2006Night.at.the.Museum chunk 1\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 2\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 3\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 4\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 5\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 6\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 7\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 8\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 9\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 15/20 [08:26<02:41, 32.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1976Rocky.I chunk 1\n",
      "âœ… Scored: 1976Rocky.I chunk 2\n",
      "âœ… Scored: 1976Rocky.I chunk 3\n",
      "âœ… Scored: 1976Rocky.I chunk 4\n",
      "âœ… Scored: 1976Rocky.I chunk 5\n",
      "âœ… Scored: 1976Rocky.I chunk 6\n",
      "âœ… Scored: 1976Rocky.I chunk 7\n",
      "âœ… Scored: 1976Rocky.I chunk 8\n",
      "âœ… Scored: 1976Rocky.I chunk 9\n",
      "âœ… Scored: 1976Rocky.I chunk 10\n",
      "âœ… Scored: 1976Rocky.I chunk 11\n",
      "âœ… Scored: 1976Rocky.I chunk 12\n",
      "âœ… Scored: 1976Rocky.I chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 16/20 [09:04<02:15, 33.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2005V.for.Vendetta chunk 1\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 2\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 3\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 4\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 5\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 6\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 7\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 8\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 9\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 10\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 11\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 12\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 13\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 14\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 15\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 17/20 [09:53<01:55, 38.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2017Paddington.2 chunk 1\n",
      "âœ… Scored: 2017Paddington.2 chunk 2\n",
      "âœ… Scored: 2017Paddington.2 chunk 3\n",
      "âœ… Scored: 2017Paddington.2 chunk 4\n",
      "âœ… Scored: 2017Paddington.2 chunk 5\n",
      "âœ… Scored: 2017Paddington.2 chunk 6\n",
      "âœ… Scored: 2017Paddington.2 chunk 7\n",
      "âœ… Scored: 2017Paddington.2 chunk 8\n",
      "âœ… Scored: 2017Paddington.2 chunk 9\n",
      "âœ… Scored: 2017Paddington.2 chunk 10\n",
      "âœ… Scored: 2017Paddington.2 chunk 11\n",
      "âœ… Scored: 2017Paddington.2 chunk 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 18/20 [10:30<01:15, 37.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1985Back.To.The.Future chunk 1\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 2\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 3\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 4\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 5\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 6\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 7\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 8\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 9\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 10\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 19/20 [11:08<00:38, 38.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2013The.Purge chunk 1\n",
      "âœ… Scored: 2013The.Purge chunk 2\n",
      "âœ… Scored: 2013The.Purge chunk 3\n",
      "âœ… Scored: 2013The.Purge chunk 4\n",
      "âœ… Scored: 2013The.Purge chunk 5\n",
      "âœ… Scored: 2013The.Purge chunk 6\n",
      "âœ… Scored: 2013The.Purge chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring good_vs_evil (OpenAI): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [11:30<00:00, 34.51s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# === OpenAI Client Setup (1.x syntax) ===\n",
    "client = OpenAI(api_key=\"api-key\")\n",
    "\n",
    "\n",
    "# === Config ===\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "# === Paths ===\n",
    "base = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "subs_dir = base / \"json_benchmark\"\n",
    "summaries_dir = base / \"summaries\"\n",
    "output_dir = base / \"scored_good_vs_evil_openai\"\n",
    "prompts_dir = base / \"prompts_good_vs_evil_openai\"\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "prompts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "matches_df = pd.read_csv(base / \"matches_benchmark.csv\")\n",
    "\n",
    "# === Prompt template ===\n",
    "def create_good_vs_evil_prompt(summary, chunk):\n",
    "    return f\"\"\"\n",
    "You are analyzing how a film portrays its central moral conflict based on its plot summary and dialogue.\n",
    "\n",
    "Your task is to classify the filmâ€™s depiction of **Good vs Evil** into one of the following three categories:\n",
    "\n",
    "- **Clear**: There is a strong moral binary. Good and evil are clearly distinguished. One side is righteous, the other is corrupt or villainous.\n",
    "- **Neutral**: The film does not emphasize good vs evil. There may be conflict, but it is not framed in strong moral terms.\n",
    "- **Complicated**: The line between good and evil is blurred. Moral ambiguity is central, and characters or institutions cannot be easily classified as good or evil.\n",
    "\n",
    "Please:\n",
    "1. Choose the best fitting label: \"clear\", \"neutral\", or \"complicated\"\n",
    "2. Give a brief justification (1â€“2 sentences)\n",
    "3. Provide a confidence score between 0.0 and 1.0\n",
    "\n",
    "Return your answer in this exact JSON format:\n",
    "\n",
    "{{\n",
    "  \"good_vs_evil\": \"...\",\n",
    "  \"confidence\": ...,\n",
    "  \"explanation\": \"...\"\n",
    "}}\n",
    "\n",
    "Film Summary:\n",
    "\\\"\\\"\\\"{summary}\\\"\\\"\\\"\n",
    "\n",
    "Dialogue:\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
    "\"\"\".strip()\n",
    "\n",
    "# === OpenAI call with updated SDK ===\n",
    "def call_openai(prompt):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a careful film analyst. Always return exactly and only the required JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Remove wrapping code fences if present\n",
    "        if content.startswith(\"```json\"):\n",
    "            content = content[7:-3].strip()\n",
    "        elif content.startswith(\"```\"):\n",
    "            content = content[3:-3].strip()\n",
    "\n",
    "        return json.loads(content)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ OpenAI API error: {e}\")\n",
    "        return None\n",
    "\n",
    "# === Subtitle chunking ===\n",
    "def chunk_dialogue(subs, chunk_size=5000, overlap=200):\n",
    "    text_blocks = [line.get(\"text\", \"\").strip() for line in subs if line.get(\"text\")]\n",
    "    full_text = \" \".join(text_blocks)\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(full_text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(full_text[start:end])\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# === Main scoring loop ===\n",
    "for _, row in tqdm(matches_df.iterrows(), total=len(matches_df), desc=\"Scoring good_vs_evil (OpenAI)\"):\n",
    "    filename = row[\"subtitle_filename\"]\n",
    "    json_path = subs_dir / f\"{filename}.json\"\n",
    "    summary_path = summaries_dir / f\"{filename}.srt_summary.txt\"\n",
    "\n",
    "    if not json_path.exists() or not summary_path.exists():\n",
    "        print(f\"âš ï¸ Missing files for: {filename}\")\n",
    "        continue\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        subs = json.load(f)\n",
    "\n",
    "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = f.read().strip()\n",
    "\n",
    "    chunks = chunk_dialogue(subs)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_path = output_dir / f\"{filename}_chunk{i+1}_good_vs_evil.json\"\n",
    "        prompt_path = prompts_dir / f\"{filename}_chunk{i+1}_good_vs_evil_prompt.txt\"\n",
    "\n",
    "        if output_path.exists():\n",
    "            print(f\"ðŸŸ¡ Already scored: {filename} chunk {i+1}\")\n",
    "            continue\n",
    "\n",
    "        prompt = create_good_vs_evil_prompt(summary, chunk)\n",
    "\n",
    "        with open(prompt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(prompt)\n",
    "\n",
    "        result = call_openai(prompt)\n",
    "        if result:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "            print(f\"âœ… Scored: {filename} chunk {i+1}\")\n",
    "        else:\n",
    "            print(f\"âŒ Failed: {filename} chunk {i+1}\")\n",
    "\n",
    "        time.sleep(1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c8bdd29-ded7-47f4-abb9-784194918bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved aggregated results to: /Users/cedricroetheli/Desktop/Benchmark/model_good_vs_evil_output_openai.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# === Paths ===\n",
    "base_path = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "scored_dir = base_path / \"scored_good_vs_evil_openai\"\n",
    "output_csv_path = base_path / \"model_good_vs_evil_output_openai.csv\"  # â† changed output filename\n",
    "\n",
    "# === Aggregate Results ===\n",
    "aggregated_results = []\n",
    "\n",
    "# Group chunk files by movie\n",
    "movie_files = defaultdict(list)\n",
    "for file in scored_dir.glob(\"*_chunk*_good_vs_evil.json\"):\n",
    "    movie_id = file.name.split(\"_chunk\")[0]\n",
    "    movie_files[movie_id].append(file)\n",
    "\n",
    "# Process each movie's chunks\n",
    "for movie_id, files in movie_files.items():\n",
    "    label_counts = Counter()\n",
    "    confidence_sums = defaultdict(float)\n",
    "    confidence_counts = defaultdict(int)\n",
    "    explanations = []\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                # === Handle optional triple-backtick wrapping (from OpenAI JSON completions) ===\n",
    "                raw = f.read().strip()\n",
    "                if raw.startswith(\"```json\"):\n",
    "                    raw = raw[7:].strip()\n",
    "                if raw.startswith(\"```\"):\n",
    "                    raw = raw[3:].strip()\n",
    "                if raw.endswith(\"```\"):\n",
    "                    raw = raw[:-3].strip()\n",
    "\n",
    "                data = json.loads(raw)\n",
    "\n",
    "                label = data.get(\"good_vs_evil\", \"\").strip().lower()\n",
    "                confidence = float(data.get(\"confidence\", 0.0))\n",
    "                explanation = data.get(\"explanation\", \"\")\n",
    "\n",
    "                label_counts[label] += 1\n",
    "                confidence_sums[label] += confidence\n",
    "                confidence_counts[label] += 1\n",
    "                explanations.append(f\"{label} ({confidence:.2f}): {explanation}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error in file {file.name}: {e}\")\n",
    "\n",
    "    # Compute average confidence per label\n",
    "    avg_confidences = {\n",
    "        label: round(confidence_sums[label] / confidence_counts[label], 3)\n",
    "        for label in label_counts\n",
    "    }\n",
    "\n",
    "    # Weighted vote = label with highest total confidence\n",
    "    weighted_vote = max(confidence_sums.items(), key=lambda x: x[1])[0] if confidence_sums else None\n",
    "\n",
    "    aggregated_results.append({\n",
    "        \"subtitle_filename\": movie_id,\n",
    "        \"label_counts\": dict(label_counts),\n",
    "        \"avg_confidences\": avg_confidences,\n",
    "        \"weighted_vote\": weighted_vote,\n",
    "        \"explanations\": explanations\n",
    "    })\n",
    "\n",
    "# === Save to CSV ===\n",
    "df = pd.DataFrame(aggregated_results)\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"âœ… Saved aggregated results to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c8d9175-f4ae-43a4-8b04-f770bdc3765e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Evaluation complete:\n",
      "âœ… Correct: 15/20\n",
      "ðŸ“Š Accuracy: 75.00%\n",
      "ðŸ“ Saved to: /Users/cedricroetheli/Desktop/Benchmark/good_vs_evil_evaluation_openai.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "base = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "truth_path = base / \"benchmark_final.csv\"\n",
    "model_path = base / \"model_good_vs_evil_output_openai.csv\"  # â† updated input\n",
    "output_path = base / \"good_vs_evil_evaluation_openai.csv\"   # â† updated output\n",
    "\n",
    "# === Load Data ===\n",
    "truth_df = pd.read_csv(truth_path)\n",
    "model_df = pd.read_csv(model_path)\n",
    "\n",
    "# === Normalize filenames ===\n",
    "truth_df[\"subtitle_filename\"] = truth_df[\"subtitle_filename\"].str.strip()\n",
    "model_df[\"subtitle_filename\"] = model_df[\"subtitle_filename\"].str.strip()\n",
    "\n",
    "# === Label Mapping for Consistency ===\n",
    "label_map = {\n",
    "    \"clear\": \"clear\",\n",
    "    \"complicated\": \"complicated\",\n",
    "    \"complex\": \"complicated\",  # âœ… map 'complex' to 'complicated'\n",
    "    \"neutral\": \"neutral\"\n",
    "}\n",
    "\n",
    "def normalize_label(label):\n",
    "    if pd.isna(label):\n",
    "        return \"\"\n",
    "    label = label.lower().strip()\n",
    "    return label_map.get(label, label)\n",
    "\n",
    "def normalize_set(label_str):\n",
    "    if pd.isna(label_str):\n",
    "        return set()\n",
    "    parts = [normalize_label(part) for part in label_str.split(\"|\")]\n",
    "    return set(parts)\n",
    "\n",
    "# === Apply normalization ===\n",
    "truth_df[\"good_vs_evil_set\"] = truth_df[\"good_vs_evil\"].apply(normalize_set)\n",
    "model_df[\"normalized_vote\"] = model_df[\"weighted_vote\"].apply(normalize_label)\n",
    "\n",
    "# === Merge and Evaluate ===\n",
    "merged_df = pd.merge(model_df, truth_df, on=\"subtitle_filename\", how=\"inner\")\n",
    "merged_df[\"is_correct\"] = merged_df.apply(\n",
    "    lambda row: row[\"normalized_vote\"] in row[\"good_vs_evil_set\"], axis=1\n",
    ")\n",
    "\n",
    "# === Output CSV ===\n",
    "evaluation_df = merged_df[[\n",
    "    \"subtitle_filename\", \"good_vs_evil\", \"normalized_vote\", \"is_correct\"\n",
    "]].copy()\n",
    "evaluation_df.columns = [\"movie\", \"benchmark_good_vs_evil\", \"model_good_vs_evil\", \"is_correct\"]\n",
    "evaluation_df.to_csv(output_path, index=False)\n",
    "\n",
    "# === Summary ===\n",
    "total = len(evaluation_df)\n",
    "correct = evaluation_df[\"is_correct\"].sum()\n",
    "accuracy = correct / total if total else 0\n",
    "\n",
    "print(f\"ðŸŽ¯ Evaluation complete:\")\n",
    "print(f\"âœ… Correct: {correct}/{total}\")\n",
    "print(f\"ðŸ“Š Accuracy: {accuracy:.2%}\")\n",
    "print(f\"ðŸ“ Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa336149-ab2f-4388-89a6-925df3b8df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):   0%|                | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2006Blood.Diamond chunk 1\n",
      "âœ… Scored: 2006Blood.Diamond chunk 2\n",
      "âœ… Scored: 2006Blood.Diamond chunk 3\n",
      "âœ… Scored: 2006Blood.Diamond chunk 4\n",
      "âœ… Scored: 2006Blood.Diamond chunk 5\n",
      "âœ… Scored: 2006Blood.Diamond chunk 6\n",
      "âœ… Scored: 2006Blood.Diamond chunk 7\n",
      "âœ… Scored: 2006Blood.Diamond chunk 8\n",
      "âœ… Scored: 2006Blood.Diamond chunk 9\n",
      "âœ… Scored: 2006Blood.Diamond chunk 10\n",
      "âœ… Scored: 2006Blood.Diamond chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):   5%|â–       | 1/20 [00:51<16:21, 51.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2005The.Constant.Gardener chunk 1\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 2\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 3\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 4\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 5\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 6\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 7\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 8\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 9\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 10\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 11\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 12\n",
      "âœ… Scored: 2005The.Constant.Gardener chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  10%|â–Š       | 2/20 [01:49<16:34, 55.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 1\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 2\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 3\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 4\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 5\n",
      "âœ… Scored: 1981Indiana.Jones.And.The.Raiders.Of.The.Lost.Ark chunk 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  15%|â–ˆâ–      | 3/20 [02:16<11:57, 42.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2009Avatar chunk 1\n",
      "âœ… Scored: 2009Avatar chunk 2\n",
      "âœ… Scored: 2009Avatar chunk 3\n",
      "âœ… Scored: 2009Avatar chunk 4\n",
      "âœ… Scored: 2009Avatar chunk 5\n",
      "âœ… Scored: 2009Avatar chunk 6\n",
      "âœ… Scored: 2009Avatar chunk 7\n",
      "âœ… Scored: 2009Avatar chunk 8\n",
      "âœ… Scored: 2009Avatar chunk 9\n",
      "âœ… Scored: 2009Avatar chunk 10\n",
      "âœ… Scored: 2009Avatar chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  20%|â–ˆâ–Œ      | 4/20 [02:54<10:48, 40.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2012The.Hunger.Games chunk 1\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 2\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 3\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 4\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 5\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 6\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 7\n",
      "âœ… Scored: 2012The.Hunger.Games chunk 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  25%|â–ˆâ–ˆ      | 5/20 [03:22<09:03, 36.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1984Ghostbusters chunk 1\n",
      "âœ… Scored: 1984Ghostbusters chunk 2\n",
      "âœ… Scored: 1984Ghostbusters chunk 3\n",
      "âœ… Scored: 1984Ghostbusters chunk 4\n",
      "âœ… Scored: 1984Ghostbusters chunk 5\n",
      "âœ… Scored: 1984Ghostbusters chunk 6\n",
      "âœ… Scored: 1984Ghostbusters chunk 7\n",
      "âœ… Scored: 1984Ghostbusters chunk 8\n",
      "âœ… Scored: 1984Ghostbusters chunk 9\n",
      "âœ… Scored: 1984Ghostbusters chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  30%|â–ˆâ–ˆâ–     | 6/20 [04:01<08:37, 36.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1978Superman chunk 1\n",
      "âœ… Scored: 1978Superman chunk 2\n",
      "âœ… Scored: 1978Superman chunk 3\n",
      "âœ… Scored: 1978Superman chunk 4\n",
      "âœ… Scored: 1978Superman chunk 5\n",
      "âœ… Scored: 1978Superman chunk 6\n",
      "âœ… Scored: 1978Superman chunk 7\n",
      "âœ… Scored: 1978Superman chunk 8\n",
      "âœ… Scored: 1978Superman chunk 9\n",
      "âœ… Scored: 1978Superman chunk 10\n",
      "âœ… Scored: 1978Superman chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  35%|â–ˆâ–ˆâ–Š     | 7/20 [04:40<08:10, 37.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2008The.Hurt.Locker chunk 1\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 2\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 3\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 4\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 5\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 6\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 7\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 8\n",
      "âœ… Scored: 2008The.Hurt.Locker chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  40%|â–ˆâ–ˆâ–ˆâ–    | 8/20 [05:16<07:25, 37.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 1\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 2\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 3\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 4\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 5\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 6\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 7\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 8\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 9\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 10\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 11\n",
      "âœ… Scored: 1977Star.Wars.Episode.IV.-.A.New.Hope chunk 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  45%|â–ˆâ–ˆâ–ˆâ–Œ    | 9/20 [06:07<07:35, 41.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 1\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 2\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 3\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 4\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 5\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 6\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 7\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 8\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 9\n",
      "âœ… Scored: 2003Pirates.of.the.Caribbean.The.Curse.of.the.Black.Pearl chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  50%|â–ˆâ–ˆâ–ˆâ–Œ   | 10/20 [06:53<07:08, 42.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2018Black.Panther chunk 1\n",
      "âœ… Scored: 2018Black.Panther chunk 2\n",
      "âœ… Scored: 2018Black.Panther chunk 3\n",
      "âœ… Scored: 2018Black.Panther chunk 4\n",
      "âœ… Scored: 2018Black.Panther chunk 5\n",
      "âœ… Scored: 2018Black.Panther chunk 6\n",
      "âœ… Scored: 2018Black.Panther chunk 7\n",
      "âœ… Scored: 2018Black.Panther chunk 8\n",
      "âœ… Scored: 2018Black.Panther chunk 9\n",
      "âœ… Scored: 2018Black.Panther chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  55%|â–ˆâ–ˆâ–ˆâ–Š   | 11/20 [07:25<05:55, 39.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2021Dont.Look.Up chunk 1\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 2\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 3\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 4\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 5\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 6\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 7\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 8\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 9\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 10\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 11\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 12\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 13\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 14\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 15\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 16\n",
      "âœ… Scored: 2021Dont.Look.Up chunk 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–  | 12/20 [08:32<06:22, 47.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1982First.Blood chunk 1\n",
      "âœ… Scored: 1982First.Blood chunk 2\n",
      "âœ… Scored: 1982First.Blood chunk 3\n",
      "âœ… Scored: 1982First.Blood chunk 4\n",
      "âœ… Scored: 1982First.Blood chunk 5\n",
      "âœ… Scored: 1982First.Blood chunk 6\n",
      "âœ… Scored: 1982First.Blood chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 13/20 [08:52<04:36, 39.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2019Joker chunk 1\n",
      "âœ… Scored: 2019Joker chunk 2\n",
      "âœ… Scored: 2019Joker chunk 3\n",
      "âœ… Scored: 2019Joker chunk 4\n",
      "âœ… Scored: 2019Joker chunk 5\n",
      "âœ… Scored: 2019Joker chunk 6\n",
      "âœ… Scored: 2019Joker chunk 7\n",
      "âœ… Scored: 2019Joker chunk 8\n",
      "âœ… Scored: 2019Joker chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 14/20 [09:20<03:35, 35.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2006Night.at.the.Museum chunk 1\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 2\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 3\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 4\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 5\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 6\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 7\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 8\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 9\n",
      "âœ… Scored: 2006Night.at.the.Museum chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/20 [09:57<03:01, 36.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1976Rocky.I chunk 1\n",
      "âœ… Scored: 1976Rocky.I chunk 2\n",
      "âœ… Scored: 1976Rocky.I chunk 3\n",
      "âœ… Scored: 1976Rocky.I chunk 4\n",
      "âœ… Scored: 1976Rocky.I chunk 5\n",
      "âœ… Scored: 1976Rocky.I chunk 6\n",
      "âœ… Scored: 1976Rocky.I chunk 7\n",
      "âœ… Scored: 1976Rocky.I chunk 8\n",
      "âœ… Scored: 1976Rocky.I chunk 9\n",
      "âœ… Scored: 1976Rocky.I chunk 10\n",
      "âœ… Scored: 1976Rocky.I chunk 11\n",
      "âœ… Scored: 1976Rocky.I chunk 12\n",
      "âœ… Scored: 1976Rocky.I chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 16/20 [10:36<02:28, 37.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2005V.for.Vendetta chunk 1\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 2\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 3\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 4\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 5\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 6\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 7\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 8\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 9\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 10\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 11\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 12\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 13\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 14\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 15\n",
      "âœ… Scored: 2005V.for.Vendetta chunk 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/20 [11:30<02:07, 42.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2017Paddington.2 chunk 1\n",
      "âœ… Scored: 2017Paddington.2 chunk 2\n",
      "âœ… Scored: 2017Paddington.2 chunk 3\n",
      "âœ… Scored: 2017Paddington.2 chunk 4\n",
      "âœ… Scored: 2017Paddington.2 chunk 5\n",
      "âœ… Scored: 2017Paddington.2 chunk 6\n",
      "âœ… Scored: 2017Paddington.2 chunk 7\n",
      "âœ… Scored: 2017Paddington.2 chunk 8\n",
      "âœ… Scored: 2017Paddington.2 chunk 9\n",
      "âœ… Scored: 2017Paddington.2 chunk 10\n",
      "âœ… Scored: 2017Paddington.2 chunk 11\n",
      "âœ… Scored: 2017Paddington.2 chunk 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 18/20 [12:18<01:27, 43.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 1985Back.To.The.Future chunk 1\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 2\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 3\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 4\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 5\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 6\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 7\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 8\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 9\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 10\n",
      "âœ… Scored: 1985Back.To.The.Future chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 19/20 [13:02<00:43, 43.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scored: 2013The.Purge chunk 1\n",
      "âœ… Scored: 2013The.Purge chunk 2\n",
      "âœ… Scored: 2013The.Purge chunk 3\n",
      "âœ… Scored: 2013The.Purge chunk 4\n",
      "âœ… Scored: 2013The.Purge chunk 5\n",
      "âœ… Scored: 2013The.Purge chunk 6\n",
      "âœ… Scored: 2013The.Purge chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring progressive_women (OpenAI): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [13:23<00:00, 40.18s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# === OpenAI Setup ===\n",
    "client = OpenAI(api_key=\"api-key\")  # ðŸ”‘ Replace with your OpenAI API key\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "# === Paths ===\n",
    "base = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "subs_dir = base / \"json_benchmark\"\n",
    "summaries_dir = base / \"summaries\"\n",
    "output_dir = base / \"scored_progressive_women_openai\"\n",
    "prompts_dir = base / \"prompts_progressive_women_openai\"\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "prompts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "matches_df = pd.read_csv(base / \"matches_benchmark.csv\")\n",
    "\n",
    "# === Prompt Template ===\n",
    "def create_progressive_women_prompt(summary, chunk):\n",
    "    return f\"\"\"\n",
    "You are evaluating how progressively women are portrayed in a film based on its plot summary and dialogue.\n",
    "\n",
    "Your task is to classify the film into one of the following categories based on the roles, agency, and representation of women:\n",
    "\n",
    "- **Yes**: The film offers a clearly progressive portrayal of women. Female characters are active agents, hold central narrative roles, and are treated with depth and respect.\n",
    "- **Mixed**: The portrayal includes both progressive and stereotypical elements. Some female characters may show agency, while others may be sidelined or underdeveloped.\n",
    "- **No**: The film does not offer a progressive portrayal. Female characters are absent, objectified, stereotyped, or lack meaningful agency.\n",
    "\n",
    "Please:\n",
    "1. Choose the best label: \"yes\", \"mixed\", or \"no\"\n",
    "2. Provide a short justification (1â€“2 sentences)\n",
    "3. Include a confidence score between 0.0 and 1.0\n",
    "\n",
    "Return your answer in this exact JSON format:\n",
    "\n",
    "{{\n",
    "  \"progressive_women\": \"...\",\n",
    "  \"confidence\": ...,\n",
    "  \"explanation\": \"...\"\n",
    "}}\n",
    "\n",
    "Film Summary:\n",
    "\\\"\\\"\\\"{summary}\\\"\\\"\\\"\n",
    "\n",
    "Dialogue:\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
    "\"\"\".strip()\n",
    "\n",
    "# === OpenAI API Call ===\n",
    "def call_openai(prompt):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a careful film analyst. Always return exactly and only the required JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        return json.loads(content.strip(\"```json\\n\").strip(\"```\"))\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ OpenAI error: {e}\")\n",
    "        return None\n",
    "\n",
    "# === Subtitle Chunking ===\n",
    "def chunk_dialogue(subs, chunk_size=5000, overlap=200):\n",
    "    text_blocks = [line.get(\"text\", \"\").strip() for line in subs if line.get(\"text\")]\n",
    "    full_text = \" \".join(text_blocks)\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(full_text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(full_text[start:end])\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# === Scoring Loop ===\n",
    "for _, row in tqdm(matches_df.iterrows(), total=len(matches_df), desc=\"Scoring progressive_women (OpenAI)\"):\n",
    "    filename = row[\"subtitle_filename\"]\n",
    "    json_path = subs_dir / f\"{filename}.json\"\n",
    "    summary_path = summaries_dir / f\"{filename}.srt_summary.txt\"\n",
    "\n",
    "    if not json_path.exists() or not summary_path.exists():\n",
    "        print(f\"âš ï¸ Missing files for: {filename}\")\n",
    "        continue\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        subs = json.load(f)\n",
    "\n",
    "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = f.read().strip()\n",
    "\n",
    "    chunks = chunk_dialogue(subs)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_path = output_dir / f\"{filename}_chunk{i+1}_progressive_women.json\"\n",
    "        prompt_path = prompts_dir / f\"{filename}_chunk{i+1}_progressive_women_prompt.txt\"\n",
    "\n",
    "        if output_path.exists():\n",
    "            print(f\"ðŸŸ¡ Already scored: {filename} chunk {i+1}\")\n",
    "            continue\n",
    "\n",
    "        prompt = create_progressive_women_prompt(summary, chunk)\n",
    "\n",
    "        with open(prompt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(prompt)\n",
    "\n",
    "        result = call_openai(prompt)\n",
    "        if result:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "            print(f\"âœ… Scored: {filename} chunk {i+1}\")\n",
    "        else:\n",
    "            print(f\"âŒ Failed: {filename} chunk {i+1}\")\n",
    "\n",
    "        time.sleep(1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9c23d00-f3dc-4203-9c73-851e441675af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved aggregated results to: /Users/cedricroetheli/Desktop/Benchmark/model_progressive_women_output_openai.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# === Paths ===\n",
    "base_path = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "scored_dir = base_path / \"scored_progressive_women_openai\"\n",
    "output_csv_path = base_path / \"model_progressive_women_output_openai.csv\"\n",
    "\n",
    "# === Aggregate Results ===\n",
    "aggregated_results = []\n",
    "\n",
    "# Group chunk files by movie\n",
    "movie_files = defaultdict(list)\n",
    "for file in scored_dir.glob(\"*_chunk*_progressive_women.json\"):\n",
    "    movie_id = file.name.split(\"_chunk\")[0]\n",
    "    movie_files[movie_id].append(file)\n",
    "\n",
    "# Process each movie's chunks\n",
    "for movie_id, files in movie_files.items():\n",
    "    label_counts = Counter()\n",
    "    confidence_sums = defaultdict(float)\n",
    "    confidence_counts = defaultdict(int)\n",
    "    explanations = []\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                label = data.get(\"progressive_women\", \"\").strip().lower()\n",
    "                confidence = float(data.get(\"confidence\", 0.0))\n",
    "                explanation = data.get(\"explanation\", \"\")\n",
    "\n",
    "                label_counts[label] += 1\n",
    "                confidence_sums[label] += confidence\n",
    "                confidence_counts[label] += 1\n",
    "                explanations.append(f\"{label} ({confidence:.2f}): {explanation}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error in file {file.name}: {e}\")\n",
    "\n",
    "    # Compute average confidence per label\n",
    "    avg_confidences = {\n",
    "        label: round(confidence_sums[label] / confidence_counts[label], 3)\n",
    "        for label in label_counts\n",
    "    }\n",
    "\n",
    "    # Weighted vote = label with highest total confidence\n",
    "    weighted_vote = max(confidence_sums.items(), key=lambda x: x[1])[0] if confidence_sums else None\n",
    "\n",
    "    aggregated_results.append({\n",
    "        \"subtitle_filename\": movie_id,\n",
    "        \"label_counts\": dict(label_counts),\n",
    "        \"avg_confidences\": avg_confidences,\n",
    "        \"weighted_vote\": weighted_vote,\n",
    "        \"explanations\": explanations\n",
    "    })\n",
    "\n",
    "# === Save to CSV ===\n",
    "df = pd.DataFrame(aggregated_results)\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"âœ… Saved aggregated results to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00dd1ca3-7376-4e1e-8383-c4e7e76bebed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Evaluation complete:\n",
      "âœ… Correct: 15/20\n",
      "ðŸ“Š Accuracy: 75.00%\n",
      "ðŸ“ Saved to: /Users/cedricroetheli/Desktop/Benchmark/progressive_women_evaluation_openai.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "base = Path.home() / \"Desktop\" / \"Benchmark\"\n",
    "truth_path = base / \"benchmark_final.csv\"\n",
    "model_path = base / \"model_progressive_women_output_openai.csv\"\n",
    "output_path = base / \"progressive_women_evaluation_openai.csv\"\n",
    "\n",
    "# === Load Data ===\n",
    "truth_df = pd.read_csv(truth_path)\n",
    "model_df = pd.read_csv(model_path)\n",
    "\n",
    "# === Normalize filenames ===\n",
    "truth_df[\"subtitle_filename\"] = truth_df[\"subtitle_filename\"].str.strip()\n",
    "model_df[\"subtitle_filename\"] = model_df[\"subtitle_filename\"].str.strip()\n",
    "\n",
    "# === Label Mapping for Consistency ===\n",
    "label_map = {\n",
    "    \"yes\": \"yes\",\n",
    "    \"no\": \"no\",\n",
    "    \"mixed\": \"mixed\",\n",
    "    \"partially\": \"mixed\",\n",
    "    \"partly\": \"mixed\"\n",
    "}\n",
    "\n",
    "def normalize_label(label):\n",
    "    if pd.isna(label):\n",
    "        return \"\"\n",
    "    label = label.lower().strip()\n",
    "    return label_map.get(label, label)\n",
    "\n",
    "def normalize_set(label_str):\n",
    "    if pd.isna(label_str):\n",
    "        return set()\n",
    "    parts = [normalize_label(part) for part in label_str.split(\"|\")]\n",
    "    return set(parts)\n",
    "\n",
    "truth_df[\"progressive_women_set\"] = truth_df[\"progressive_women\"].apply(normalize_set)\n",
    "model_df[\"normalized_vote\"] = model_df[\"weighted_vote\"].apply(normalize_label)\n",
    "\n",
    "# === Merge and Evaluate ===\n",
    "merged_df = pd.merge(model_df, truth_df, on=\"subtitle_filename\", how=\"inner\")\n",
    "merged_df[\"is_correct\"] = merged_df.apply(\n",
    "    lambda row: row[\"normalized_vote\"] in row[\"progressive_women_set\"], axis=1\n",
    ")\n",
    "\n",
    "# === Output CSV ===\n",
    "evaluation_df = merged_df[[\n",
    "    \"subtitle_filename\", \"progressive_women\", \"normalized_vote\", \"is_correct\"\n",
    "]].copy()\n",
    "evaluation_df.columns = [\"movie\", \"benchmark_progressive_women\", \"model_progressive_women\", \"is_correct\"]\n",
    "evaluation_df.to_csv(output_path, index=False)\n",
    "\n",
    "# === Summary ===\n",
    "total = len(evaluation_df)\n",
    "correct = evaluation_df[\"is_correct\"].sum()\n",
    "accuracy = correct / total if total else 0\n",
    "\n",
    "print(f\"ðŸŽ¯ Evaluation complete:\")\n",
    "print(f\"âœ… Correct: {correct}/{total}\")\n",
    "print(f\"ðŸ“Š Accuracy: {accuracy:.2%}\")\n",
    "print(f\"ðŸ“ Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5bfe8-d960-4b24-b152-2dd15a2802fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_clean_env_py310)",
   "language": "python",
   "name": "my_clean_env_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
